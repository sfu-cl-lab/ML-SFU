
# - date: ""
#   speaker: ""
#   speakerUrl: ""
#   speakerPhoto: false
#   video: ""
#   title: ""
#   abstract: ""
#   bio: ""
#   tags: []

- date: "2025-10-31"
  location: TASC1 9204 12:00pm
  speaker: Matthew Alun Brown
  speakerUrl: https://mattabrown.github.io/
  speakerInfo: Principal Scientist from Wayve
  speakerPhoto: matthew_brown.jpg
  title: Science Frontiers at Wayve
  abstract: This talk will explore some of the frontier efforts that Wayve's Science team is pursuing to unlock the puzzle of scalable embodied AI. These include the integration of Vision and Language models (LINGO), Generative World Models (GAIA) and Spatial Intelligence (Rig3R) in end-to-end self-driving. I'll begin with an overview of Science team efforts, and then take a deeper dive into our recent work on Spatial Intelligence. Throughout the talk we'll see how Wayve is approaching complex and unstructured environments from London to Tokyo using a data-driven and end to end approach.
  bio: Matt's career has spanned multiple roles at the intersection of research and deployment in Vision + ML. Prior to Wayve, he was a Research Scientist at Google DeepMind / Google Research, an Associate Professor (Reader) at the University of Bath, Founder and CTO of Cloudburst Research Inc., and a Postdoctoral Researcher at EPFL, UBC and Microsoft Research. His work on panoramic stitching features in many computer vision courses and won the ICCV Helmholtz Prize (test of time award) in 2017.
  video: https://stream.sfu.ca/Media/Play/d0d46ffcf9c4475fad38787f63f1e4591d


- date: "2025-10-17"
  location: TASC1 9204 12:00pm
  speaker: Xiang (Sam) Xu
  speakerUrl: https://samxuxiang.github.io/
  speakerInfo: Senior Research Scientist at Autodesk AI Lab
  speakerPhoto: xiang_xu.jpg
  title: "Building Reality: Generative AI for Design and Make"
  abstract: Recent advances in generative AI have transformed how we create text, imagery, and video—but the connection to physical, manufacturable objects remains underdeveloped. This talk explores how AI can move beyond pixels and words to generate designs for things that can actually be built, manufactured, or constructed in the real world. We will focus on 3D Computer-Aided Design (CAD) as a critical domain for bridging digital intelligence and physical creation. The talk will also introduce AutoBrep, Autodesk’s world-first CAD-native foundation model that autoregressively generates CAD models through a unified sequence of topology and geometry tokens, guided by multimodal signals such as images and text. Together, these advances point toward a future where generative AI is not just creative—but constructible.
  bio: Xiang Xu is a Senior Research Scientist at Autodesk AI Lab in Vancouver, Canada. His research focuses on generative models, 3D computer vision, and machine learning. He earned his PhD in Computer Science from Simon Fraser University under Prof Yasutaka Furukawa, where he worked on controllable CAD generation. He is passionate about 3D foundation models and AI-driven controllable content creation.

- date: "2025-09-26"
  location: TASC1 9204 12:00pm
  speaker: Wuyang Chen
  speakerUrl: https://www.sfu.ca/fas/computing/people/faculty/faculty-members/wuyang-chen.html
  speakerInfo: Assistant Professor at Simon Fraser University
  speakerPhoto: wuyang_chen.jpg
  title: Hybrid Learning Machines Bridge AI and Physical Modeling
  html: false
  abstract: "Data-driven AI models scale impressively, yet they still struggle with rigorous modeling of the physical world. In this talk, we present hybrid machine learning models that bridge AI with physical modeling through two complementary principles. The first is tool interaction: rather than embedding physical knowledge directly into data-driven models, we enable machine learning systems to actively interact with external physical modeling tools. The second is physics-enriched data scaling: instead of merely increasing the size of training datasets, we enrich data with physically informative feedback derived from modeling and simulation. We demonstrate that these principles are broadly compatible with both language and vision systems, and show their impact across diverse domains, from scientific reasoning and theorem proving to simulations of complex 4D environments."
  bio: Dr. Wuyang Chen is a tenure-track Assistant Professor in Computing Science at Simon Fraser University. Previously, he was a postdoctoral researcher in Statistics at the University of California, Berkeley, advised by Professor Michael Mahoney. He obtained his Ph.D. in Electrical and Computer Engineering from the University of Texas at Austin in 2023, advised by Professor Atlas Wang. Dr. Chen’s research focuses on integrating AI methods with physical knowledge, scientific machine learning, and theoretical understanding of deep networks. Dr. Chen has published papers at CVPR, ECCV, ICLR, ICML, NeurIPS, and other top conferences. Dr. Chen’s research has been recognized by the US NSF newsletter, two Doctoral Dissertation Awards from INNS and iSchools, AAAI New Faculty Highlights, and Nvidia Academic Grant Award. Dr. Chen is the host of the Foundation Models for Science workshop at NeurIPS 2024 and co-organized the 4th and 5th versions of the UG2+ workshop and challenge at CVPR in 2021 and 2022.
  video: https://stream.sfu.ca/Media/Play/353a5a83fbae427b8a59f631fd6230671d

- date: "2025-09-05"
  location: TASC1 9204 12:00pm
  speaker: Mathias Lécuyer
  speakerUrl: https://mathias.lecuyer.me/
  speakerInfo: Assistant Professor at the University of British Columbia   
  speakerPhoto: mathias_lecuyer.jpg
  title: Adversarial Robustness and Privacy Measurements using Hypothesis-tests
  html: true
  abstract: "ML theory usually considers model behaviour in expectation. In practical AI deployments however, we often expect models to be robust to adversarial perturbations, in which a user applies deliberate changes to on input to influence the prediction a target model. For instance, such attacks have been used to jailbreak aligned foundation models out of their normal behaviour. Given the complex models that we now deploy, how can we enforce such robustness properties while keeping model flexibility and utility?<br/>I will present recent work on Adaptive Randomized Smoothing (ARS), an approach we developed to certify the predictions of test-time adaptive models against adversarial examples. ARS extends the analysis of randomized smoothing using f-Differential Privacy, to certify the adaptive composition of multiple steps during model prediction. We show how to instantiate ARS on deep image classification to certify predictions against adversarial examples of bounded L∞ norm.<br/>If time permits, I will also connect f-Differential Privacy's hypothesis testing view of privacy to the audit of data leakage in large AI models. Specifically, I will discuss a new data leakage measurement technique we developed, that does not require access to in-distribution non-member data. This is particularly important in the age of foundation models, often trained on all available data at a given time. It is also related to recent efforts in detecting data use in large AI models, a timely question at the intersection of AI and intellectual property."
  bio: Mathias Lécuyer is an Assistant Professor at the University of British Columbia. He works on trustworthy AI, on topics ranging from privacy, robustness, explainability, and causality, with a specific focus on applications that provide rigorous guarantees. Recent impactful contributions include the first scalable defence against adversarial examples with provable guarantees (now called randomized smoothing), as well as system support for differential privacy accounting, for which he received a Google Research Scholar award.
  video: https://stream.sfu.ca/Media/Play/bb0fa69cf00f404da8c5c844b66fe6871d

- date: "2025-08-15"
  location: TASC1 9204 12:00pm
  speaker: "Minhyuk Sung"
  speakerUrl: "https://mhsung.github.io/"
  speakerInfo: "Associate Professor in the School of Computing at KAIST"
  speakerPhoto: minhyuk_sung.jpg
  title: Inference-Time Guided Generation Using Diffusion and Flow models
  html: true
  abstract: "Recent breakthroughs in generative AI have transformed the creative process, making it easier than ever to generate realistic images and videos. While the quality of generated outputs has reached unprecedented levels of realism, the current challenge lies in improving controllability and alignment with user preferences. To address this, recent advancements in LLMs have shifted the focus beyond scaling training data and model size to scaling inference-time computation, as demonstrated by the AGI-level performance of models like GPT-4o and DeepSeek. In this talk, I will discuss inference-time generation techniques for guided image and video synthesis. Specifically, I will introduce particle sampling approaches that leverage the stochastic nature of the generative process, branching the generation trajectory to scale up and search for the desired output, albeit at the cost of increased inference-time computation. I will highlight their capabilities, limitations, and potential future directions."
  bio: "Minhyuk Sung is an Associate Professor in the School of Computing at KAIST, affiliated with the Graduate Schools of AI and Metaverse. Previously, he was a Research Scientist at Adobe Research. He earned his Ph.D. from Stanford University under Prof. Leonidas J. Guibas. His research focuses on generating, manipulating, and analyzing various visual data, including images, videos, and 3D data. He has served on program committees for SIGGRAPH Asia (2022–2025), Eurographics (2022, 2024–2025), Pacific Graphics (2023, 2025), ICCV (2025), ICLR (2025), NeurIPS (2025), and AAAI (2023, 2024). He received the Asia Graphics Young Researcher Award in 2024."
  video: https://stream.sfu.ca/Media/Play/25de538dc3334ba19db1f7f3951d97341d

- date: "2025-08-15"
  location: TASC1 9204
  speaker: Chenxi Liu
  speakerUrl: https://chenxil21.github.io/
  speakerInfo: "Postdoctoral researcher in the Dynamic Graphics Project at the University of Toronto"
  speakerPhoto: chenxi_liu.jpg
  title: "From Sketch to Style: Artistic Analysis and Creation via Computation"
  abstract: "Tools for visual creation have evolved from traditional media to digital drawing tablets and, more recently, to text-to-image generation, enabling broader creative access while raising new challenges in processing artistic data, such as sketches and stylized content. In the first part of this talk, I’ll give a quick overview of my research on processing vector sketch data. In the second part, I’ll shift focus to artistic styles, and in particular to Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method widely used to customize image diffusion models with individual styles. LoRA has become popular in online creative communities due to its low resource requirements, while also raising concerns around attribution and ownership. To better understand the relationship between LoRA weights and the adapted styles, we analyze the weights directly in the parameter space, avoiding the need for image generation and prompt engineering. We find that the LoRA weights themselves serve as effective style representations, capturing fine-grained stylistic differences and outperforming image-based features in clustering and retrieval. Beyond analysis, merging LoRA weights to synthesize new styles is a common practice. However, existing tools rely on manual sliders, which are often inefficient and unintuitive. In our ongoing project, we adapt a human-in-the-loop Bayesian optimization framework to our setting for more effective interactions."
  bio: "Chenxi Liu is a postdoctoral researcher in the Dynamic Graphics Project at the University of Toronto, working with Alec Jacobson. Chenxi's research focuses on computational methods for understanding and assisting visual creation, including recent work on LoRA-based style analysis, 2D neural fields, and sketch processing. Chenxi holds a Ph.D. from the University of British Columbia under the supervision of Alla Sheffer, and has interned at Adobe Research and Disney Research."
  video: https://stream.sfu.ca/Media/Play/1a8f614abf9d41bf919aaa8c3329f08d1d

- date: "2025-07-24"
  location: TASC1 9204 10:30am
  speaker: Daniel Cohen-Or
  speakerUrl: https://danielcohenor.com/
  speakerPhoto: daniel_cohenor.jpg
  speakerInfo: Professor in the School of Computer Science at Tel-Aviv University
  title: Attention Mechanism in Generative Models
  abstract: The rapid evolution of Large-scale Text-to-Image (T2I) models has revolutionized creative domains by generating visually captivating outputs from textual prompts. Attention layers play a critical role in these generative models. In my talk, I will show that these layers capture rich semantic information, and particularly semantic correspondences between elements within the image and across different images. Through several works, I will show that the rich representations learned by these layers can be leveraged for various image manipulations, and consistent image generation. 
  bio: Daniel Cohen-Or is a professor in the School of Computer Science at Tel-Aviv University. He received his B.Sc. cum laude in both mathematics and computer science (1985), and M.Sc. cum laude in computer science (1986) from Ben-Gurion University, and Ph.D. from the Department of Computer Science (1991) at State University of New York at Stony Brook. He was sitting on the editorial board of a number of international journals, and a member of the program committees of several international conferences. He was the recipient of the Eurographics Outstanding Technical Contributions Award in 2005. In 2013 he received The People’s Republic of China Friendship Award. In 2015 he was named a Thomson Reuters Highly Cited Researcher? He received the ACM SIGGRAPH Computer Graphics Achievement Award in 2018. In 2019 he won The Kadar Family Award for Outstanding Research. In 2020, he received The Eurographics Distinguished Career Award. His research interests are in computer graphics, in particular, synthesis, processing and modelling techniques.


- date: "2025-07-18"
  location: TASC1 9204 12:00pm
  speaker: "Ajmal Mian"
  speakerUrl: "https://ajmalsaeed.net/"
  speakerInfo: "Professor of Computer Science at University of Western Australia"
  speakerPhoto: ajmal_saeed_mian.jpg
  title: 3D scene reconstruction, generation and understanding
  abstract: "In this talk, I will present datasets, methods, and code developed in our lab to support research in 3D computer vision. I will begin with a brief overview of our current research projects, and then focus on one project centered around 3D scene reconstruction, generation, and understanding. I will introduce our CUDA-based deep learning library designed for 3D surface parsing. This library features mesh-specific convolutional operations and a parallel algorithm for mesh decimation. Next, I will discuss our work on sketch-conditioned 3D object and indoor scene generation, where we leverage external knowledge to improve scene realism and coherence. While text and images are commonly used as inputs for 3D generation, we argue that hand-drawn sketches offer more intuitive alignment with user intent. I will also touch on the unique challenges of outdoor 3D scene generation, which suffers from greater complexity and data scarcity. To address this, our lab has released several large-scale city-level 3D LiDAR datasets, which support outdoor scene generation and understanding. Finally, I will present our approach for generating outdoor 3D point clouds from hand-drawn bird’s-eye-view sketches. To enable this, we align terrestrial LiDAR point clouds with satellite imagery and process the satellite views to produce sketch-like renderings suitable for training the generative model."
  bio: "Ajmal Mian is a Professor of Computer Science at the University of Western Australia. He has received three esteemed national fellowships from the Australian Research Council. He is Fellow of the International Association for Pattern Recognition, Distinguished Speaker of the Association for Computing Machinery, and former President of the Australian Pattern Recognition Society. Ajmal Mian has been honoured with awards like the West Australian Early Career Scientist of the Year (2012) and Mid-Career Scientist of the Year (2022). He is the lead Investigator on several AI projects from the Australian Research Council and the Australian Department of Defence. He was also the lead Australian PI on two DARPA projects. He has served as a Senior Editor for the IEEE Transactions on Neural Networks and Learning Systems and Associate Editor of IEEE Transactions on Image Processing, Pattern Recognition and IET Computer Vision. He has supervised 36 PhD students to completion, mentored 12 postdoctoral fellows and published over 350 scientific papers."

- date: "2025-07-16"
  location: TASC1 9204 10:00am
  speaker: Bo Han
  speakerUrl: https://bhanml.github.io/
  speakerInfo: "Associate Professor in Machine Learning at Hong Kong Baptist University, and a BAIHO Visiting Scientist at RIKEN AIP"
  speakerPhoto: bo_han.jpg
  title: "Exploring Trustworthy Foundation Models: Benchmarking, Finetuning and Reasoning"
  abstract: In the current landscape of machine learning, where foundation models must navigate imperfect real-world conditions such as noisy data and unexpected inputs, ensuring their trustworthiness through rigorous benchmarking, safety-focused finetuning, and robust reasoning is more critical than ever. In this talk, I will focus on three recent research advancements that collectively advance these dimensions, offering a comprehensive approach to building trustworthy foundation models. For benchmarking, I will introduce CounterAnimal, a dataset designed to systematically evaluate CLIP’s vulnerability to realistic spurious correlations, revealing that scaling models or data quality can mitigate these biases, yet scaling data alone does not effectively address them. Transitioning to finetuning, we delve deep into the process of unlearning undesirable model behaviors. We propose a general framework to examine and understand the limitations of current unlearning methods and suggest enhanced revisions for more effective unlearning. Furthermore, addressing reasoning, we investigate the reasoning robustness under noisy rationales by constructing the NoRa dataset and propose contrastive denoising with noisy chain-of-thought, a method that markedly improves denoising-reasoning capabilities by contrasting noisy inputs with minimal clean supervision. Furthermore, l will introduce the newly established Trustworthy Machine Learning and Reasoning (TMLR) Group at Hong Kong Baptist University.
  bio: Bo Han is an Associate Professor in Machine Learning at Hong Kong Baptist University, and a BAIHO Visiting Scientist at RIKEN AIP. He was a Visiting Research Scholar at MBZUAI MLD, a Visiting Faculty Researcher at Microsoft Research, and a Postdoc Fellow at RIKEN AIP. He received his Ph.D. degree in Computer Science from University of Technology Sydney. He has served as Senior Area Chair of NeurIPS, and Area Chairs of NeurIPS, ICML and ICLR. He has also served as Associate Editors of IEEE TPAMI, MLJ and JAIR, and Editorial Board Members of JMLR and MLJ. He received Outstanding Paper Award at NeurIPS, Most Influential Paper at NeurIPS, and Outstanding Student Paper Award at NeurIPS Workshop. He received the RGC Early CAREER Scheme, IEEE AI's 10 to Watch Award, IJCAI Early Career Spotlight, INNS Aharon Katzir Young Investigator Award, RIKEN BAIHO Award, Dean's Award for Outstanding Achievement, and Microsoft Research StarTrack Scholars Program.

- date: "2025-07-10"
  location: TASC1 9204 12:00pm
  speaker: "Evan Shelhamer"
  speakerUrl: "https://imaginarynumber.net/research/"
  speakerInfo: "Assistant Professor at UBC and a member of the Vector Institute."
  speakerPhoto: evan_shelhamer.jpg
  title: Test-Time Updates - Next Steps for Robust and Efficient Vision
  html: true
  abstract: "Learning from data is essential for state-of-the-art vision, but learning once is not be enough. When the data shifts between training and testing the accuracy of predictions can degrade. The problem is that the data changes, but our systems remain the same. Does it have to stay this way? In this talk we will examine how visual recognition can adapt and generalize to new and different data during testing. We will cover natural shifts, such as image corruptions, to highlight opportunities to update models (by entropy minimization and parameter mixing) and to update inputs (by diffusion) when the data differs. Then we will take a critical look at adversarial attacks through our case study of test-time adaptive defenses. As a last step, we will discuss test-time updates as a kind of multi-modeling—or how to apply multiple models at the same time—and summarize progress on efficiency, uncertainty, and adaptivity by multi-modeling."
  bio: "Evan Shelhamer is an assistant professor at UBC in Vancouver, a member of the Vector Institute, and a CIFAR AI chair. He earned his PhD at UC Berkeley in 2019 advised by Prof. Trevor Darrell. While there he was the lead developer of the Caffe open-source deep learning framework from version 0.1 to 1.0. Since then he worked as a research scientist at Google DeepMind and Adobe before returning to academia. His research and service have received international awards, including for fully convolutional networks (best paper honorable mention at CVPR'15, test-of-time at CVPR'25), and for Caffe (the Mark Everingham award at ICCV'17, the open-source award at MM'14, and the test-of-time at MM'24). He likes to brew coffee and community, and his latest organizing efforts include the 2nd workshop on test-time adaptation at ICML'25 and the 3rd workshop on machine learning for remote sensing at ICLR'25."
  video: https://stream.sfu.ca/Media/Play/65fa88e328fa46f9a6d2f4c5ff5e3e1a1d

- date: "2025-05-09"
  location: TASC1 9204 12:00pm
  speaker: "Maleknaz Nayebi"
  speakerUrl: "https://lassonde.yorku.ca/users/mnayebi"
  speakerInfo: "Associate Professor at York University"
  speakerPhoto: maleknaz_nayebi.jpg
  title: Machines Multimodal Learning from Generic Problem Descriptions to Design Better Software
  html: true
  abstract: Modern software development is increasingly data-rich but context-poor—particularly in its early stages, where the goal is to define problems rather than simply solve them. Although developers have access to vast repositories of community knowledge machine learning models still struggle to generalize across domains or respond to novel user needs without the crutch of large, labeled datasets. This challenge is especially pronounced in requirements engineering, where understanding user intent across diverse modalities—including text, visuals, and regulatory artifacts—is critical. <p> In this talk, I introduce Discovery-Driven Learning (DDL), a methodology aimed at improving machine comprehension of generic and often ambiguous problem descriptions, without relying on memorized or narrowly scoped training data. Grounded in the principles of READI (Reading-based Extraction and Analysis of Design Intent), our approach combines multimodal grounding, language-conditioned policy learning, and just-in-time reasoning to teach machines how to interpret requirements from textual, visual, and structural inputs. <p> This need for generalizability becomes even more pressing in the earlier phases of development, where traditional elicitation and inception techniques fall short of scaling across application domains. I will present how our research addresses this gap by enabling more adaptable, explainable, and human-in-the-loop-friendly AI systems. The talk will highlight real-world applications in digital health, aviation, emergency response, and software automation—demonstrating how DDL can power the next generation of intelligent co-pilots in software engineering and beyond.
  bio: "Maleknaz Nayebi is an Associate Professor at York University in Toronto, where she leads a research lab focused on automated techniques that bridge the requirements gap—enhancing software development practices and deepening our understanding of user needs. She completed her PhD at the University of Calgary, followed by a postdoctoral fellowship at the University of Toronto, before joining York as faculty in 2018. <p> Her research, particularly impactful in healthcare, is published in top-tier venues and has been supported by multiple funding agencies. In recognition of her contributions to software engineering, Nayebi received the prestigious IEEE TCSE Award at the 2023 Requirements Engineering Conference. She was also invited by NSERC to serve on the Computer Science Evaluation Group—a distinction reserved for select researchers across Canada. <p> Currently, she serves as the Application Chair for the NSERC Discovery Evaluation Group (1507 – Computer Science) and as Associate Director of CIFAL York. Her leadership extends to several interdisciplinary initiatives: she is a board member of the York Centre for Feminist Research (CFR), the Centre for Innovation in Computing at Lassonde (IC@L), and Y-EMERGE (York Emergency Mitigation, Engagement, Response, and Governance Institute), and an active member of Connected Minds and York’s Center of AI for Society. To date, Nayebi has secured 16 research grants as a principal investigator or co-investigator."


- date: "2025-04-11"
  location: TASC1 9204 11:00am
  speaker: "Roozbeh Mottaghi"
  speakerUrl: "https://roozbehm.info/"
  speakerInfo: "Senior AI Research Scientist Manager at Meta Fundamental AI Research (FAIR)"
  speakerPhoto: roozbeh_mottaghi.jpg
  title: Embodied AI at Scale
  abstract: The remarkable progress in AI across domains like language and computer vision has been largely fueled by the use of large-scale datasets for training models. However, achieving a similar transformative impact in the field of Embodied AI remains a challenge, primarily due to scaling difficulties. In this talk, I will delve into approaches that facilitate scaling in Embodied AI, including the utilization of simulation environments and the use of internet video resources. Additionally, I will discuss methods for scaling not only generic training data but also the creation and scaling of datasets, with a focus on developing a large-scale benchmark for embodied planning and reasoning in dynamic environments. I will present a comprehensive study on how state-of-the-art planning models struggle with these tasks and discuss methods for distilling knowledge from models trained on large-scale data into smaller models that are suitable for deployment on robots.
  bio: Roozbeh Mottaghi is a Senior Research Scientist Manager at FAIR and an Affiliate Associate Professor in the Paul G. Allen School of Computer Science and Engineering at the University of Washington. Before joining FAIR, he was the Research Manager of the Perceptual Reasoning and Interaction Research (PRIOR) group at the Allen Institute for AI (AI2). He completed his master’s degrees at Simon Fraser University and Georgia Tech and earned his PhD in Computer Science from the University of California, Los Angeles, in 2013. After his PhD, he joined the Computer Science Department at Stanford University as a post-doctoral researcher. His research primarily focuses on embodied AI, reasoning through perception, and learning via interaction. His work on large-scale Embodied AI received the Outstanding Paper Award at NeurIPS 2022.
  video: https://stream.sfu.ca/Media/Play/34b3bbc58e314b04b7caa31fc543c5bd1d

- date: "2025-02-21"
  location: TASC1 9204 11:00am
  speaker: "Tian Li"
  speakerUrl: "https://litian96.github.io/"
  speakerInfo: "Assistant Professor at the Computer Science Department and the Data Science Institute at the University of Chicago"
  speakerPhoto: tian_li.jpg
  title: "Tilted Empirical Risk Minimization"
  abstract: "Exponential tilting is a technique commonly used in fields such as statistics, probability, information theory, and optimization to shift distributions in a controlled manner. In this talk, I introduce its usage in machine learning by exploring the use of tilting in risk minimization. The tilted empirical risk minimization (TERM) framework is a simple extension of ERM, which uses exponential tilting to flexibly tune the impact of individual losses. I motivate the tilted objective from real-world use-cases on addressing representation disparity in federated learning. I describe several useful theoretical properties of TERM including its connections with other non-ERM objectives, and a multitude of applications regarding fair and robust ML. I will conclude the talk by discussing ongoing work on leveraging this framework to improve sharpness-aware optimization, as well as generalization error analysis of TERM." 
  bio: "Tian Li is an Assistant Professor at the Computer Science Department and Data Science Institute at the University of Chicago. Her research interests are in optimization, trustworthy machine learning, and large-scale learning. She has spent time at Microsoft Research Asia, Google Research, and Meta Foundational AI Research Labs. She was invited to participate in the EECS Rising Stars Workshop, and was recognized as a Rising Star in Machine Learning/Data Science by multiple institutions. Her team won the first place in the Privacy-Enhancing Technologies (PETs) Challenge featured by the White House. She received her PhD in Computer Science from Carnegie Mellon University and BS degrees in Computer Science and Economics from Peking University."
  video: "https://stream.sfu.ca/Media/Play/2732bfb8c3fd4a6b9adce1c5ff1182891d"

- date: "2025-02-14"
  location: TASC1 9204 12:00pm
  speaker: "Vladimir (Vova) Kim"
  speakerUrl: "http://www.vovakim.com/"
  speakerInfo: "Senior Research Scientist at Adobe Research"
  speakerPhoto: vova_kim.jpg
  title: "Controllable 3D Generation"
  abstract: "State-of-the-art generative models enable users to create complex 3D shapes from sparse and partial inputs, such as text or images. These methods, however, still lack wide adoption in the professional artist community due to a lack of precise and rich controls enabled by traditional tools. In this talk I will describe several methods that aim to complement traditional workflows by providing AI-assisted tools to perform tasks that are commonly done by artists, such as iterative sculpting, deformation, detailization and refinement, and material painting. Our techniques especially focus on traditional representations (such as meshes and material maps) to ensure that they can be easily integrated in existing pipelines."
  bio: "Dr. Vladimir (Vova) Kim is a Senior Research Scientist at Adobe Research. He works at the intersection of computer graphics, computer vision, and machine learning, developing techniques that enable novel interfaces for creative tasks. His recent focus is on novel neural representations for geometric data with applications to interactive 3D modeling, geometry analysis, geometry processing, and other content creation workflows. He was a postdoctoral scholar at Stanford University (2013–2015), received his PhD in the Computer Science Department at Princeton University (2008-2013), and his BA in Mathematics and Computer Science at Simon Fraser University (2003-2008). He has been a member of the International Program Committee for SIGGRAPH (2015, 2020, 2023), SIGGRAPH Asia (2022), Symposium on Geometry processing, Eurographics, and regularly reviews for graphics and vision venues, such as Transaction on Graphics, CVPR, ICCV, ECCV, and NeurIPS."
  video: "https://stream.sfu.ca/Media/Play/1e12b9d7a1384a7f937bbc378706e0a41d"

- date: "2025-02-07"
  location: TASC1 9204 12:00pm
  speaker: "Scott C. Lowe"
  speakerUrl: "https://scottclowe.com/"
  speakerInfo: "Postdoctoral research fellow at Vector Institute"
  speakerPhoto: scott_lowe.jpg
  title: "BIOSCAN-5M: Multimodal biodiversity monitoring with images and DNA barcodes"
  abstract: "Measuring biodiversity is crucial for understanding global ecosystem health, especially in the face of anthropogenic environmental changes. Rates of data collection are ever increasing, but access to expert human annotation is limited, making this an ideal use-case for machine learning solutions. We present BIOSCAN-5M, a newly released dataset with 5 million samples of paired images and DNA barcodes which enables multimodal modelling for insect biodiversity data. Harnessing this dataset, we explore closed-world and open-world classification using pretrained encoders, masked-language models trained on the DNA barcodes, and CLIP-style pretraining on the multimodal data."
  bio: "Scott C. Lowe is a British machine learning researcher based at the Vector Institute, Canada. His work is multidisciplinary, spanning several topics. Recently he has focused on biodiversity monitoring applications, both of insects and ocean habitats; self-supervised learning; reasoning capabilities of LLMs; and symbolic music generation. Previously, he completed his PhD in Neuroinformatics from the University of Edinburgh, and has worked on logic-based activation functions."
  video: "https://stream.sfu.ca/Media/Play/08f9ff759f5c474185db044a3e1cc7131d"

- date: "2024-12-18"
  location: TASC1 9204 12:00pm
  speaker: "Siqi Zhou"
  speakerUrl: "https://siqizhou.com/"
  speakerInfo: "Senior Scientist, Technical University of Munich (TUM)"
  speakerPhoto: "siqi_zhou.jpg"
  title: "Integrating Machine Learning and Control for Safe and Efficient Robot Decision-Making"
  abstract: "Robots are envisioned to become reliable human companions in domains ranging from industrial applications to our daily lives. In the literature, well-established control techniques provide the foundation for designing high-performance autonomous robot systems with desired theoretical guarantees. However, these control techniques often rely on a dynamics model of the robot, and any inaccuracies of the model can result in suboptimal performance or even unsafe actions. This limitation motivates incorporating learning into the traditional robot decision-making software stack. In our work, departing from control theory, we develop neural control approaches that safely and efficiently exploit the expressiveness of neural networks to enhance the performance of robots in uncertain environments. This talk will encompass a set of our neural control work ranging from offline inverse dynamics learning for improving the performance of robots to online Lipschitz network adaptation for closing the model-reality gap in uncertain robot systems. We demonstrate our approach in real-time robot experiments, including quadrotor impromptu trajectory tracking and flying an inverted pendulum. In this talk, I will also introduce our review paper on safe learning in robotics and related benchmarking efforts that are targeted to provide an overarching view of the recent advances in learning-based control and reinforcement learning for robotic applications. I will conclude by sharing our latest work that closes the perception-action loop with a semantic understanding and thereby enables robots to safely operate in everyday scenarios."
  bio: "Siqi Zhou is a Senior Scientist at the Chair of Safety, Performance and Reliability for Learning Systems at the Technical University of Munich (TUM). She received her Ph.D. from the University of Toronto in 2022 and her B.A.Sc. degree from the University of Toronto Engineering Science program in 2016. Her research lies at the intersection of robotics, machine learning, and systems control."
  video: "https://stream.sfu.ca/Media/Play/daf826f7545d420193ec42e9952fbe5e1d"

# - date: "2024-12-17"
#   location: ASB 9896 2:30pm
#   speaker: "Bastian Wandt"
#   speakerUrl: "https://bastianwandt.de/"
#   speakerInfo: "Assistant Professor at Linköping University, Sweden"
#   speakerPhoto: "bastian_wandt.jpg"
#   title: "Human Motion and Dynamics Capture with Limited Data"
#   abstract: "3D human motion capture has a wide range of applications across society with the most common examples being in movie production, medicine, autonomous driving, and human-robot interaction. In recent years, rapid deep learning-driven advances in research have translated directly into commercial products. Despite significant advancements in deep learning-based research leading to successful commercialization, major challenges remain for the data-hungry machine learning approaches. Arguably the most important problem is the lack of training data covering the whole variety of human motions; it is even questionable whether such data can ever be recorded. This talk will present several solutions for monocular human motion capture that side-step the problem of missing data using weakly- or unsupervised machine learning techniques in conjunction with geometric modeling. It will also give an outlook on future applications in motion analysis by capturing human dynamics involving forces and torques inside and on the human body."
#   bio: "Bastian Wandt received his M.Sc. and Dr.-Ing. degree from the Leibniz University of Hannover in 2014 and 2020. From 2021 to 2022 he worked as a PostDoc at the University of British Columbia in Vancouver, Canada. His PostDoc research was partially funded by the German Research Foundation (DFG) under the Walter Benjamin scholarship. Since September 2022 he is an assistant professor at the Linköping University, Sweden. He is a fellow of WASP, Sweden’s largest research program."

- date: "2024-12-06"
  location: TASC1 9204 12:00pm
  speaker: "Bingshan Hu"
  speakerUrl: "https://sites.google.com/view/bingshan-hu"
  speakerInfo: "Postdoctoral researcher, Computer Science, University of British Columbia (UBC)"
  speakerPhoto: "bingshan_hu.jpg"
  title: "Efficient and Adaptive Thompson Sampling Algorithms for Bandits"
  abstract: "A Multi-armed bandit problem is a classical sequential decision-making problem in which the goal is to accumulate as much reward as possible. In this learning model, only a limited amount of information is revealed in each round. The imperfect feedback model results in the learning algorithm being in a dilemma between exploration (gaining information) and exploitation (accumulating reward). Thompson Sampling, one of the oldest randomized learning algorithms, is able to achieve a good balance between exploration and exploitation and it always has a very competitive empirical performance. We study Thompson Sampling-based algorithms for stochastic bandits with bounded rewards. As the existing problem-dependent regret bound for Thompson Sampling with Gaussian priors is vacuous when the learning horizon T ≤ 6235149080811616882909238708, we derive a more practical bound. Additionally, motivated by large-scale real-world applications that require scalability, adaptive computational resource allocation, and a balance in utility and computation, we propose a parameterized Thompson Sampling-based algorithm: Thompson Sampling with Timestamp Duelling (TS-TD-α), where we use α ∈ [0, 1] to control the trade-off between utility and computation."
  bio: "Bingshan Hu is a postdoctoral researcher co-hosted by Danica Sutherland and Mathias Lécuyer in the Department of Computer Science at the University of British Columbia. Before that, She was awarded the Amii Postdoctoral Fellowship and worked with Mark Schmidt at the University of British Columbia and Nidhi Hegde at the University of Alberta. She completed her PhD under the supervision of Nishant Mehta in the Department of Computer Science at the University of Victoria in 2021. Her research lies in the theoretical side of machine learning, aiming at devising efficient and differentially private online learning algorithms. She was recognized as one of the top 10% of high-scoring reviewers at NeurIPS 2020. Prior to pursuing her PhD studies, she worked in industry research labs as a wireless technology specialist. She invented/co-invented around 20 patents with more than half of them having been granted by either the European patent office or US patent office. Besides the foundations of online learning, she is also interested in using online learning algorithms in novel applications in wireless systems."
  video: https://stream.sfu.ca/Media/Play/61d29727793e40b49ea7379ea86c6b821d

- date: "2024-10-30"
  remote: true
  location: online 11:00am
  speaker: "Zhiwen (Aaron) Fan"
  speakerUrl: "https://zhiwenfan.github.io/"
  speakerInfo: "PhD student at UT Austin"
  speakerPhoto: "zhiwen_fan.jpg"
  title: "Empowering Machines to Understand 3D for End-to-End Reconstruction and Reliable Interaction using a Scalable Solution"
  abstract: "Large-scale models trained with internet-scale data show great promise in transforming the way we work and solve complex problems. However, while LLMs and VLMs can interpret text or single image data, they face challenges in understanding spatial relationships under existing multi-modal training paradigms. Future intelligent AI systems must be able to perceive structured 3D environments before they can reliably interact with the physical world, which is essential for autonomous systems. Current 3D reconstruction pipelines still rely on modular, non-differentiable processing systems, preventing scalable training.  My research will first delve into the geometric principles necessary for high-quality multi-view geometry. Subsequently, we aim to push the boundaries of 3D learning by developing a unified representation, termed the Large Spatial Model, for real-time, end-to-end semantic 3D reconstruction. This model will support both feed-forward reconstruction and interaction through language with minimal annotated 3D data. By enabling the training of large-scale 3D models that effectively represent structured 3D worlds in low-dimensional latent embeddings, we can leverage the knowledge from LLMs, facilitating reliable spatial reasoning capacities. Applications such as multi-modal SLAM, human avatar creation, and controllable driving scene simulation will be empowered by 3D foundation models coupled with domain-specific designs."
  bio: "Zhiwen (Aaron) Fan is a PhD candidate at UT Austin, advised by Prof. Zhangyang (“Atlas”) Wang. He was funded by the Qualcomm Innovation Fellowship in 2022 and is currently a research intern at Meta Core AI developing 3D foundation modeling tools. Zhiwen has also interned at NVIDIA and Google, and previously worked as a senior research engineer at Alibaba Group."
  video: https://stream.sfu.ca/Media/Play/1b4667b9e38145428f954b401d4179331d

- date: "2024-10-18"
  location: TASC1 9204 12:00pm
  speaker: "Greg Mori"
  speakerUrl: "https://www.cs.sfu.ca/~mori/"
  speakerInfo: "VP, RBC Fellow at RBC Borealis"
  speakerPhoto: "greg_mori.jpg"
  title: "Foundation Model Challenges and Opportunities in Financial Services"
  abstract: "Financial services are at the core of our economy.  Opportunities for machine learning abound in this space, from capital markets to insurance services to wealth management to lending to tools that assist clients in managing their money.  Modern machine learning methods have transformed industries, yet particular challenges exist in realizing the full potential of machine learning in financial services.  These include explainability, data imbalance, partial observations, distribution shift, and self-supervised learning in low-signal settings.  I will describe the ATOM foundation model, which specializes in learning from asynchronous event sequences, to maximally utilize the richness of transactional data in financial services."
  bio: "Greg Mori is VP, RBC Fellow at RBC Borealis, where he leads AI Research and Innovation.  He is also an Adjunct Professor in the School of Computing Science at Simon Fraser University.  He received a Ph.D. in Computer Science from UC Berkeley in 2004 and an Hon. B.Sc. in Computer Science and Mathematics from the University of Toronto in 1999.  He was a Visiting Scientist at Google in Mountain View, California in 2014-2015. He served as Director of the School of Computing Science at Simon Fraser University from 2015-2018.  Dr. Mori conducts research in computer vision and machine learning.  He received the ICCV Helmholtz Prize in 2017.  He was a Program Chair for CVPR 2020 and a General Chair for CVPR 2023.  At RBC Borealis his team builds AI-based products for financial services.  These include the award-winning NOMI Forecast and numerous other industry-leading machine learning solutions."
  tags: ["ai", "finance"]


- date: "2024-10-11"
  location: TASC1 9204 12:00pm
  speaker: "Derek Liu"
  speakerUrl: "https://www.dgp.toronto.edu/~hsuehtil/"
  speakerInfo: "Senior Research Scientist at Roblox Research"
  speakerPhoto: "derek_liu.jpg"
  video: "https://stream.sfu.ca/Media/Play/bebc10a7b6a14a3ab3a782e3cb0d19941d"
  title: "Differentiable Constructive Solid Geometry and Fuzzy Logic"
  abstract: "Constructive Solid Geometry (CSG) models complex 3D objects by combining simple primitive shapes with boolean operations. It plays a central role in 3D content creation, such as being a popular modeling paradigm for many industrial design and engineering applications. However, performing optimization on the CSG representation (e.g., training a neural network to output CSG trees) remains a challenging task. This is because the traditional formulation is not differentiable and often requires expensive combinatorial optimization. In this talk, we will show how we can draw inspiration from Fuzzy Logic and make CSG differentiable from the first principle. This enables us to optimize CSG shapes simply with e.g. gradient descent. We believe this could open up new ways to revisit CSG from a different lens."
  bio: "Hsueh-Ti Derek Liu is a Senior Research Scientist at Roblox Research, working on digital geometry processing. Derek's work mainly focuses on developing easy-to-use tools for 3D content creation and level-of-detail algorithms for processing geometric data. Derek received his MS from Carnegie Mellon University and PhD at the University of Toronto. He was the recipient of the PhD dissertation award from Eurographics and the Alain Fournier award in Canada."
  tags: ["graphics"]

- date: "2024-09-27"
  location: TASC1 9204 12:00pm
  speaker: "Konrad Tollmar"
  speakerUrl: "https://www.kth.se/profile/konrad"
  speakerInfo: "Research Director at Electronic Arts"
  speakerPhoto: "konrad_tollmar.jpg"
  title: "We see farther: AI and ML Research at SEED / EA"
  abstract: "Electronic Arts has always been driven by 'We see farther' - a vision of pushing boundaries and exploring new horizons for computer games. At SEED - a pioneering group within Electronic Arts - we combine creativity with applied research. In this talk, I'll share how we get games smarter — by creating characters that feel more alive, generating new worlds on the fly, and making sure the game responds in interesting ways to how players interact with it. You'll get a look at how we're pushing the boundaries of game design and how AI is helping to shape the future interactive entertainment."
  bio: "Dr. Konrad Tollmar is the Head of Research at SEED Electronic Arts. His research interests include innovative AI for interactive entertainment, computer vision, machine learning, computer graphics, and perception-based human-computer interfaces. Prior to EA, he was an Associate Professor at KTH and headed the Mobile Service Lab, a visiting Professor at TUB Telekom Innovation Laboratories, a faculty member at LU Ingvar Kamprad Design Centre, a Research Scientist at MIT CSAIL, and a Postdoc at MIT AI Lab."
  tags: ["graphics"]

- date: "2024-09-13"
  location: TASC1 9204 12:00pm
  speaker: "Torsten Schaub"
  speakerUrl: "https://www.cs.uni-potsdam.de/~torsten/"
  speakerInfo: "Professor at University of Potsdam"
  speakerPhoto: "torsten_schaub.jpg"
  video: "https://stream.sfu.ca/Media/Play/a05041276a3d419b96d4ab5d7dfeb1561d"
  title: "Routing and Scheduling in Answer Set Programming applied to Multi-Agent Path Finding"
  abstract: "We present alternative approaches to routing and scheduling in Answer Set Programming (ASP), and explore them in the context of Multi-agent Path Finding. The idea is to capture the flow of time in terms of partial orders rather than time steps attached to actions and fluents. This also abolishes the need for fixed upper bounds on the length of plans. The trade-off for this avoidance is that (parts of) temporal trajectories must be acyclic, since multiple occurrences of the same action or fluent cannot be distinguished anymore. While this approach provides an interesting alternative for modeling routing, it is without alternative for scheduling since fine-grained timings cannot be represented in ASP in a feasible way. This is different for partial orders that can be efficiently handled by external means such as acyclicity and difference constraints. We formally elaborate upon this idea and present several resulting ASP encodings. Finally, we demonstrate their effectiveness via an empirical analysis."
  bio: "Torsten Schaub received his diploma and dissertation in informatics in 1990 and 1992, respectively, from the Technical University of Darmstadt, Germany, and his habilitation in informatics in 1995 from the University of Rennes I, France. From 1990 to 1993 he was a research assistant at the Technical University at Darmstadt. From 1993 to 1995, he was a research associate at IRISA/INRIA at Rennes. In 1995 he became University Professor at the University of Angers. Since 1997, he is University Professor for knowledge processing and information systems at the University of Potsdam. From 2014 to 2019, Torsten Schaub held an Inria International Chair at Inria Rennes - Bretagne Atlantique. Torsten Schaub has become a fellow of the European Association for Artificial Intelligence EurAI in 2012. From 2014 to 2019 he served as President of the Association of Logic Programming and was program (co-)chair of LPNMR'09, ICLP'10, ECAI'14, and upcoming KR'25. The research interests of Torsten Schaub range from the theoretic foundations to the practical implementation of reasoning from incomplete, inconsistent, and evolving information. His particular research focus lies on Answer set programming and materializes at potassco.org, the home of the open source project Potassco bundling software for Answer Set Programming developed at the University of Potsdam. Last but not least, Torsten Schaub is managing and scientific director at Potassco Solutions GmbH."
  tags: ["ai", "multi-agent path planning"]

- date: "2024-07-04"
  location: TASC1 9204 12:00pm
  speaker: "Konrad Schindler"
  speakerUrl: "https://prs.igp.ethz.ch/group/people/person-detail.schindler.html"
  speakerInfo: "Professor at ETH Zurich"
  speakerPhoto: "konrad_schindler.jpg"
  video: "https://stream.sfu.ca/Media/Play/2ac329a2e737489ba9a31bec0c6746e61d"
  title: "Towards off-the-shelf monocular depth estimators"
  abstract: "With enough knowledge about the world we are looking at, a single image is enough to recover the 3D structure of the scene in front of our eye(s) - when we close one eye we still perceive the world in 3D. Computer vision started to explore monocular depth estimation (“monodepth”) much later than depth from stereo images (or, equivalently, camera motion). It is only recently that computational methods for monodepth have reached a comparable maturity. In the talk, I will briefly recap the development of monodepth algorithms over the past 20 years, and present our own contributions to the topic. I will argue that we may be reaching the point where monodepth modules become a commodity that we download and deploy off-the-shelf, just like stereo reconstruction."
  bio: "Konrad Schindler received a Ph.D. degree from Graz University of Technology (Austria) in 2003. He was a Photogrammetric Engineer in the private industry and held research positions at Graz University of Technology; Monash University (Melbourne, Australia) and ETH Zürich (Switzerland). He was appointed Assistant Professor of Image Understanding at TU Darmstadt (Germany) in 2009. Since 2010, he has been a tenured Professor of Photogrammetry and Remote Sensing at ETH Zürich. His research interests include computer vision and machine learning, with a focus on applications in 3D modelling and in Earth observation."
  tags: ["vision", "monocular depth estimation"]

- date: "2024-06-28"
  location: TASC1 9204 12:00pm
  speaker: "Danica Sutherland"
  speakerUrl: "https://djsutherland.ml"
  speakerPhoto: false
  speakerInfo: "Assistant professor at UBC, Computer Science"
  video: "https://stream.sfu.ca/Media/Play/b10ef76e9d9647cfa352f852cc1227001d"
  title: "Scaling Graph Transformers with Expander Graphs"
  abstract: "Following their runaway success in modelling natural language and other areas, there has been much recent attention to different variants of Transformers for graph data, which may be more able to handle long-range dependencies than previous variants of graph neural networks. These models generally allow all nodes to attend to all other nodes, however, leading to both computational and statistical challenges. This talk will present two models that help with this problem. Exphormer adds “virtual nodes” as well as augmenting the attention mechanism with an expander graph, whose mathematical characteristics help propagate long-range information at cost only linear in the size of the graph. Incorporating this into the GraphGPS framework realized state-of-the-art performance on several interesting datasets. For relatively-dense input graphs, however, even attention on the original graph structure may be too expensive to run for especially large graphs. We demonstrate that even low-width graph Transformers can provide useful signal as to which attention pathways are important in a large graph, and use this to propose Spexphormer, a model which scales Exphormers to very large graphs in transductive settings."
  bio: "Danica Sutherland is an Assistant Professor at UBC Computer Science, and a Canada CIFAR AI Chair at Amii. She previously did a PhD at Carnegie Mellon, a postdoc at University College London’s Gatsby unit, and was a research assistant professor at TTI-Chicago. Her research in general focuses on understanding and improving representation learning, the integration of kernel methods with deep learning, statistical learning theory, and methods for statistical testing about probability distributions. Her knowledge of graph neural networks comes mostly from her PhD student Hamed Shirzad (lead author on both papers in this talk), an SFU CS alum."
  tags: ["ai", "graph", "transformer"]

- date: "2024-06-07"
  speaker: "Kailas Vodrahalli"
  speakerUrl: "https://stanford.edu/~kailasv/"
  speakerPhoto: "kailas_vodrahalli.jpg"
  speakerInfo: "PhD student at Stanford"
  video: "https://stream.sfu.ca/Media/Play/b5779cce64cb48f5a93c5ba4f8dff4431d"
  title: "Optimizing AI Models for Human Use"
  abstract: "In many applications, machine learning models are deployed for human use (e.g., a diagnostic assistant for clinicians). Over the last 2 years, this setting has become even more widespread with the adoption of large language models (LLMs) as assistants in a wide variety of contexts spanning both professional and non-professional disciplines. In this talk, we explore through a set of case studies how we might incorporate humans in the ML training paradigm. These case studies include (1) applying classical computer vision models in a clinical trial study to assistant patients in taking higher quality medical images in telemedicine, (2) using an empirical model for human behavior to fine-tune a generic classifier to align with human utilization, and (3) analyzing human interaction patterns with image generation models. We also present some initial results that are inspired by the methods of (2) to the large language model setting. Across these studies, we take the position and discuss why it is imperative to treat humans as part of the model optimization process."
  bio: "Kailas Vodrahalli is a 5th year Ph.D. student at Stanford working with Prof. James Zou where he is generously supported by an NSF Graduate Fellowship and a Stanford Graduate Fellowship. He is interested in designing end-to-end optimized ML systems, spanning across hardware and software. Most recently, he has focused on this problem in the context of human-AI interaction, where his research addresses how we might design systems that include a human user in the model design and optimization. He has explored these problems in various contexts including (1) LLMs, where he has worked on questions related to user personalization and LLM output interpretability; (2) VLMs, where he has investigated how users interact with VLMs over time; and (3) medicine, where he has worked on developing machine learning-based, primarily computer vision, algorithms for use in healthcare-related problems with an end-goal of deploying solutions for clinical use."
  tags: ["ai", "human-ai interaction"]


- date: "2024-05-10"
  speaker: "Jun Jin"
  speakerUrl: "https://www.ece.ualberta.ca/~jjin5/"
  speakerPhoto: "jun_jin.jpg"
  speakerInfo: "Assistant professor at University of Alberta, Electrical and Computer Engineering"
  video: "https://stream.sfu.ca/Media/Play/29713be176e24189bf10bd142eb6b3891d"
  title: "Seeking Universal Computing Models for Robotics -- A rethink on geometry, robotic reinforcement learning, and embodied AI."
  abstract: "From visual servoing, ProMPs, and DMPs to reinforcement learning and embodied artificial intelligence, we’ve seen how these computational models excel across various levels of problems in robotics -- from task specification and motion planning to sensorimotor control and reasoning. But, is there a universal computing model for robotics that solves all the problems and its capabilities regarding motion dexterity, environment perception and reasoning, and human-robot interaction scale up with the size of a full spectrum of datasets? In this talk, I will share my research journey on multiple attempts to build general-purpose robotic task solvers. I will start from my early efforts of developing geometric task specification methods, to real-world robotic reinforcement learning solutions, and finally, to our recent work on embodied reasoning, which connects multi-modal LLM to robotics. I aim to demonstrate why, despite progress, these solutions have yet to fully meet our expectations, and I invite a lively discussion on these findings."
  bio: "Dr. Jun Jin is a newly joined assistant professor in the ECE department at the University of Alberta and a Fellow of the Alberta Machine Intelligence Institute (Amii). His research focuses on robotic reinforcement learning, which intersects with embodied artificial intelligence, the theory of predictive coding, and open-ended learning agents. Dr. Jin completed his PhD in Computing Science at the University of Alberta in 2021. His work on real-world robotic reinforcement learning was a top-3-finalist of the Outstanding Student Paper Award at ICRA 2022, which is the flagship conference of the IEEE Robotics and Automation Society. He was the recipient of the KUKA Innovation Award global top-5 finalist and invited to live demos at Hannover Messe 2018. Prior to his academic career, he accrued eight years of professional experience in the ICT and construction/mining industries. Jun loves exploring the beautiful trail system in the City of Edmonton. He is enthusiastic about camping and hiking in the Rocky Mountains."
  tags: ["robotics"]


- date: "2024-04-12"
  speaker: "Renjie Liao"
  speakerUrl: "https://lrjconan.github.io/"
  speakerPhoto: "renjie_liao.jpg"
  speakerInfo: "Assistant professor at UBC, Electrical and Computer Engineering"
  video: "https://stream.sfu.ca/Media/Play/e8bb96c3ff594411a6f2f923c3241f481d"
  title: "Graph Neural Networks Meet Spectral Graph Theory: A Case Study"
  abstract: "In this talk, we explore the synergy between Graph Neural Networks (GNNs) and Spectral Graph Theory, presenting two advancements: 1) Specformer, a novel spectral GNN architecture, and 2) in-distribution (ID) and out-of-distribution (OOD) generalization bounds for GNNs based on spectral graph theory. In Specformer, we improve traditional spectral GNNs by incorporating a set-to-set spectral filter with self-attention mechanisms, achieving expressiveness while maintaining permutation equivariance. Specformer significantly outperforms existing models in synthetic and real-world datasets, demonstrating a better ability to capture complex spectral patterns. Concurrently, we significantly tighten the PAC-Bayes in-distribution generalization bounds for GNNs and provide new out-of-distribution generalization bounds based on spectral graph theory. Our empirical validations confirm that GNNs achieve strong size generalization performance in cases guaranteed by our theory."
  bio: "Renjie Liao is an assistant professor in the Department of Electrical and Computer Engineering and an associated member of the Department of Computer Science at the University of British Columbia (UBC). He is a faculty member at the Vector Institute and a Canada CIFAR AI Chair. Before joining UBC, he was a Visiting Faculty Researcher at Google Brain, working with Geoffrey Hinton and David Fleet. He received his Ph.D. in 2021 from the University of Toronto under the supervision of Richard Zemel and Raquel Urtasun. During his Ph.D., he worked as a Senior Research Scientist at Uber Advanced Technologies Group. He is broadly interested in machine learning and its intersection with computer vision, self-driving, healthcare, and other areas, with a focus on probabilistic and geometric deep learning"
  tags: ["graph neural networks", "ai"]

- date: "2024-02-23"
  speaker: "Jiaoyang Li"
  speakerUrl: "https://jiaoyangli.me/"
  speakerPhoto: "jiaoyang_li.jpg"
  speakerInfo: "Assistant professor at CMU, Robotics Instutitute of School of Computer Science"
  video: "https://stream.sfu.ca/Media/Play/38eea7a250b644a7a3e55b8fb167d1f21d"
  title: "Layout Design for Large-Scale Multi-Robot Coordination"
  abstract: "Today, thousands of robots are navigating autonomously in warehouses, transporting goods from one location to another. While numerous planning algorithms are developed to coordinate robots more efficiently and robustly, warehouse layouts remain largely unchanged – they still adhere to the traditional pattern designed for human workers rather than robots. In this talk, I will share our recent progress in exploring layout design and optimization to enhance large-scale multi-robot coordination. I will first introduce a direct layout design method, followed by a method to optimize layout generators instead of layouts. I will then extend these ideas to virtual layout design, which does not require changes to the physical world that robots navigate and thus has the potential for applications beyond automated warehouses."
  bio: "Jiaoyang Li is an assistant professor at the Robotics Institute of CMU School of Computer Science. She received her Ph.D. in computer science from the University of Southern California (USC) in 2022. Her research interests lie in the coordination of large robot teams. Her research received recognition through prestigious paper awards (e.g., best student paper, best demo, and best student paper nomination at ICAPS in 2020, 2021, and 2023, along with the best paper finalist at MRS in 2023) and competition championships (e.g., winners of NeurIPS Flatland Challenge in 2020 and Flatland 3 in 2021, as well as the League of Robot Runners sponsored by Amazon Robotics in 2023). Her Ph.D. dissertation also received the best dissertation awards from ICAPS, AAMAS, and USC in 2023."
  tags: ["multi-agent path planning", "robotics", "aaai workshop"]


- date: "2024-02-23"
  speaker: "Sven Koenig"
  speakerUrl: "http://idm-lab.org/"
  speakerPhoto: "sven_koenig.jpg"
  speakerInfo: "Professor at University of Southern California, Computer Science"
  video: "https://stream.sfu.ca/Media/Play/720e288f386645ea9faf1d20462abbd21d"
  title: "Multi-Agent Path Finding and Its Applications"
  abstract: "The coordination of robots and other agents becomes more and more important for industry. For example, on the order of one thousand robots already navigate autonomously in Amazon fulfillment centers to move inventory pods all the way from their storage locations to the picking stations that need the products they store (and vice versa). Optimal and even some approximately optimal path planning for these robots is NP-hard, yet one must find high-quality collision-free paths for them in real-time. Algorithms for such multi-agent path-finding problems have been studied in robotics and theoretical computer science for a longer time but are insufficient since they are either fast but of insufficient solution quality or of good solution quality but too slow. In this talk, I will discuss different variants of multi-agent path-finding problems, cool ideas for both solving them and executing the resulting plans robustly, and several of their applications. Our research on this topic has been funded by both NSF and Amazon Robotics."
  bio: "Sven Koenig is a professor of computer science at the University of Southern California. Most of his current research focuses on planning for single agents (such as robots) or multi-agent systems. Additional information about him can be found on his webpages: idm-lab.org."
  tags: ["multi-agent path planning", "robotics", "aaai workshop"]

- date: "2024-02-23"
  speaker: "Vahid Babaei"
  speakerUrl: "https://aidam.mpi-inf.mpg.de/?view=people_vahid"
  speakerPhoto: "vahid_babaei.jpg"
  speakerInfo: "Max Planck Institute for Informatics"
  video: "https://stream.sfu.ca/Media/Play/720e288f386645ea9faf1d20462abbd21d"
  title: "Inverse Design with Neural Surrogate Models"
  abstract: "The digitalization of manufacturing is turning fabrication hardware into computers. As traditional tools, such as computer aided design, manufacturing, and engineering (CAD/CAM/CAE) lag behind this new paradigm, the field of computational fabrication has recently emerged from computer graphics to address this knowledge gap with a computer-science mindset. Computer graphics is extremely powerful in creating content for the virtual world. The connection is therefore a natural one as the digital fabrication hardware is starving for innovative content. In this talk, I will focus on inverse design, a powerful paradigm of content synthesis for digital fabrication, which creates fabricable designs given the desired performances. Specifically, I will discuss a class of inverse design problems that deals with data-driven neural surrogate models. These surrogates learn and replace a forward process, such as a computationally heavy simulation."
  bio: "Vahid Babaei leads the AI aided Design and Manufacturing group at the Computer Graphics Department of the Max Planck Institute for Informatics in Saarbrücken, Germany. He was a postdoctoral researcher at the Computational Design and Fabrication Group of Computer Science and Artificial Intelligence Laboratory (CSAIL) at MIT. He obtained his PhD in Computer Science from EPFL. Vahid Babaei is the recipient of the 2023 Germany-wide Curious Mind Award in the area of ‘AI, Digitalization, and Robotics’, the Hermann Neuhaus Prize of the Max Planck Society, and two postdoctoral fellowships awarded by the Swiss National Science Foundation. He is interested in developing original computer science methods for both engineering design and advanced manufacturing."
  tags: ["ai", "aaai workshop"]


- date: "2024-02-23"
  speaker: "Levi Lelis"
  speakerUrl: "https://webdocs.cs.ualberta.ca/~santanad/"
  speakerPhoto: "levi_lelis.jpg"
  speakerInfo: "Assistant Professor at University of Alberta"
  video: "https://stream.sfu.ca/Media/Play/75343ff8725b4f078386a1759da8fff41d"
  title: "Learning Options by Extracting Programs from Neural Networks"
  abstract: "In this talk, I argue for a programmatic mindset in reinforcement learning, proposing that agents should generate libraries of programs encoding reusable behaviors. When faced with a new task, the agent learns how to combine existing programs and generate new ones. This approach can be helpful even when policies are encoded in seemingly non-decomposable representations like neural networks. I will show that neural networks with piecewise linear activation functions can be mapped to a program with if-then-else structures. Such a program can then be easily decomposed into sub-programs with the same input type of the original network. In the case of networks encoding policies, each sub-program can be seen as an option---a temporally extended action. All these sub-programs form a library of agent behaviors that can be reused later, in downstream tasks. Considering that even small networks can encode a large number of sub-programs, we select sub-programs that are likely to generalize to unseen tasks. This is achieved through a subset selection procedure that minimizes the Levin loss. Empirical evidence from challenging exploration scenarios in two grid-world domains demonstrates that our methodology can extract helpful programs, thus speeding up the learning process in tasks that are similar and yet distinct from the one used to train the original model."
  bio: "Dr. Levi Lelis is an Assistant Professor at the University of Alberta, an Amii Fellow, and a CIFAR AI Chair. Levi’s research is dedicated to the development of principled algorithms to solve combinatorial search problems. These problems are integral to optimizing tasks in various sectors. Levi’s research group is focused on combinatorial search problems arising from the search for programmatic solutions---computer programs written in a domain-specific language encoding problem solutions. Levi believes that the most promising path to creating agents that learn continually, efficiently, and safely is to represent the agents’ knowledge programmatically. While programmatic representations offer many advantages, including modularity and reusability, they present a significant challenge: the need to search over large, non-differentiable spaces not suited for gradient descent methods. Addressing this challenge is the current focus of Levi’s work."
  tags: ["ai", "aaai workshop"]

- date: "2024-01-26"
  speaker: "Jamie Shotton"
  speakerUrl: "https://jamie.shotton.org/"
  speakerPhoto: "jamie_shotton.jpg"
  speakerInfo: "Wayve"
  video: "https://stream.sfu.ca/Media/Play/9c11692a8b6141f88f0fcb601bbe0cdd1d"
  title: "Frontiers in Embodied AI for Autonomous Driving"
  abstract: "Over the last decade, fundamental advances in AI have driven unprecedented progress across many disciplines and applications. And yet, despite significant progress, autonomous vehicles are still far from mainstream even after billions of dollars of investment. In this talk we’ll explore what’s been holding progress back, and how by adopting a modern embodied AI approach to the problem, Wayve is finally unlocking the potential of autonomous driving in complex and unstructured urban environments such as central London. We’ll also explore some of our latest research in multimodal learning to combine the power of large language models with the driving problem (LINGO-1), and in generative world models as learned simulators trained to predict the future conditioned on ego action (GAIA-1)."
  bio: "Jamie Shotton is a leader in AI research and development, with a track record of incubating transformative new technologies and experiences from early stage research to shipping product. He is Chief Scientist at Wayve, building foundation models for embodied intelligence, such as GAIA and LINGO, to enable safe and adaptable autonomous vehicles. Prior to this he was Partner Director of Science at Microsoft and head of the Mixed Reality & AI Labs where he shipped foundational features including body tracking for Kinect and the hand- and eye-tracking that enable HoloLens 2’s instinctual interaction model. He has explored applications of AI in autonomous driving, mixed reality, virtual presence, human-computer interaction, gaming, robotics, and healthcare. He has received multiple Best Paper and Best Demo awards at top-tier academic conferences, and the Longuet-Higgins Prize test-of-time award at CVPR 2021. His work on Kinect was awarded the Royal Academy of Engineering’s gold medal MacRobert Award in 2011, and he shares Microsoft’s Outstanding Technical Achievement Award for 2012 with the Kinect engineering team. In 2014 he received the PAMI Young Researcher Award, and in 2015 the MIT Technology Review Innovator Under 35 Award. He was awarded the Royal Academy of Engineering’s Silver Medal in 2020. He was elected a Fellow of the Royal Academy of Engineering in 2021."
  tags: ["autonomous driving"]

- date: "2024-01-12"
  speaker: "Issam Laradji"
  speakerUrl: "https://issamlaradji.github.io/"
  speakerPhoto: "issam_laradji.jpg"
  speakerInfo: "ServiceNow Research / UBC"
  video: "https://stream.sfu.ca/Media/Play/255c6d1694614b11ad0a70141d3472601d"
  title: "Exploring the Landscape of Large Language Models: Methods, Challenges, and Future Possibilities"
  abstract: "In this presentation, we’ll start by introducing Large Language Models (LLMs) like BERT and BART, explaining their core architecture and significant impacts across various domains. We’ll then conduct a thorough survey of the current state-of-the-art, highlighting the latest breakthroughs and practical applications. However, it is equally crucial to acknowledge the inherent limitations of LLMs, including their substantial computational costs, safety concerns related to biases, and factuality issues where they might generate incorrect information. These constraints drive ongoing research trends like prompt engineering and the development of foundational models. Finally, we’ll take a glimpse into the future and discuss potential applications, including the exciting prospect of AI “copilots” that can assist in complex tasks."
  bio: "Issam Laradji is a research scientist who works at ServiceNow Research and also works as an adjunct professor at the University of British Columbia. He focuses on creating artificial intelligence (AI) systems that can automate a lot of difficult tasks without needing much human input. He has studied at McGill University and the University of British Columbia, specializing in areas like understanding and generating human language, analyzing images and videos, and improving how AI systems work. Issam is really enthusiastic about bringing people who are interested in AI together and making AI technology easier for everyone to use. To help with this, he has developed various tools like Haven-AI, which gives people the ability to create their own AI solutions for many different real-world problems and research projects."
  tags: ["llm", "ai"]

- date: "2023-10-27"
  speaker: "Tat-Jun Chin"
  speakerUrl: "https://researchers.adelaide.edu.au/profile/tat-jun.chin"
  speakerPhoto: "tatjun_chin.jpg"
  speakerInfo: "Professor at University of Adelaide"
  video: "https://stream.sfu.ca/Media/Play/81e9a38ce6b240338561d4ecbf1139101d"
  title: "Quantum Computing for Robust Fitting"
  abstract: "Many computer vision applications need to recover structure from imperfect measurements of the real world. The task is often solved by robustly fitting a geometric model onto noisy and outlier-contaminated data. However, relatively recent theoretical analyses indicate that many commonly used formulations of robust fitting in computer vision are not amenable to tractable solution and approximation. In this paper, we explore the usage of quantum computers for robust fitting. To do so, we examine the feasibility of two types of quantum computer technologies---universal gate quantum computers and quantum annealers---to solve robust fitting. Novel algorithms that are amenable to the quantum machines have been developed, and experimental results on current noisy intermediate scale quantum computers (NISQ) will be reported. Our work thus proposes one of the first quantum treatments of robust fitting for computer vision."
  bio: "Tat-Jun (TJ) Chin is SmartSat CRC Professorial Chair of Sentient Satellites at The University of Adelaide. He received his PhD in Computer Systems Engineering from Monash University in 2007, which was partly supported by the Endeavour Australia-Asia Award, and a Bachelor in Mechatronics Engineering from Universiti Teknologi Malaysia in 2004, where he won the Vice Chancellor’s Award. TJ’s research interest lies in computer vision and machine learning for space applications. He has published close to 200 research articles, and has won several awards for his research, including a CVPR award (2015), a BMVC award (2018), Best of ECCV (2018), three DST Awards (2015, 2017, 2021), an IAPR Award (2019) and an RAL Best Paper Award (2021). TJ pioneered the AI4Space Workshop series and is an Associate Editor at the International Journal of Robotics Research (IJRR) and Journal of Mathematical Imaging and Vision (JMIV). He was a Finalist in the Academic of the Year Category at Australian Space Awards 2021."
  tags: ["quantum computing", "visual computing"]

- date: "2023-10-20"
  speaker: "Leonid Sigal"
  speakerUrl: "https://www.cs.ubc.ca/~lsigal/"
  speakerPhoto: "leonid_sigal.jpg"
  speakerInfo: "Professor at UBC, Computer Science"
  video: "https://stream.sfu.ca/Media/Play/0377be26412347d5934fbf1675821edb1d"
  title: "Efficient, Less-biased and Creative Visual Learning"
  abstract: "In this talk I will discuss recent methods from my group that focus on addressing some of the core challenges of current visual and multi-modal cognition, including efficient learning, bias and user-controlled generation. Centering on these larger themes I will talk about a number of strategies (and corresponding papers) that we developed to address these challenges. I will start by discussing transfer learning techniques in the context of a semi-supervised object detection and segmentation, highlighting a model that is applicable to a range of supervision: from zero to a few instance-level samples per novel class. I will then talk about our recent work on building a foundational image representation model by combining two successful strategies of masking and sequential token prediction. I will also discuss some of our work on scene graph generation which, in addition to improving overall performance, allows for scalable inference and ability to control data bias (by trade off major improvements on rare classes for minor declines on most common classes). The talk will end with some of our recent work on generative modeling which focuses on novel-view synthesis and language-conditioned diffusion-based story generation. The core of the latter approach is visual memory that implicitly captures the actor and background context across the generated frames. Sentence-conditioned soft attention over the memories enables effective reference resolution and learns to maintain scene and actor consistency when needed."
  bio: "Prof. Leonid Sigal is a Professor at the University of British Columbia (UBC). He was appointed CIFAR AI Chair at the Vector Institute in 2019 and an NSERC Tier 2 Canada Research Chair in Computer Vision and Machine Learning in 2018. Prior to this, he was a Senior Research Scientist, and a group lead, at Disney Research. He completed his Ph.D at Brown University in 2008; received his B.Sc. degrees in Computer Science and Mathematics from Boston University in 1999, his M.A. from Boston University in 1999, and his M.S. from Brown University in 2003. He was a Postdoctoral Researcher at the University of Toronto, between 2007-2009. Leonid’s research interests lie in the areas of computer vision, machine learning, and computer graphics; with the emphasis on approaches for visual and multi-modal representation learning, recognition, understanding and generative modeling. He has won a number of prestigious research awards, including Killam Accelerator Fellowship in 2021 and has published over 100 papers in venues such as CVPR, ICCV, ECCV, NeurIPS, ICLR, and Siggraph."
  tags: ["visual learning", "visual computing"]

- date: "2023-10-13"
  speaker: "Li Cheng"
  speakerUrl: "http://www.ece.ualberta.ca/~lcheng5/"
  speakerPhoto: "li_cheng.jpg"
  speakerInfo: "Professor at University of Alberta, Electrical and Computer Engineering"
  video: "https://stream.sfu.ca/Media/Play/e1bf6d3177ed4f3288ad7ddde49f286a1d"
  title: "Visual Human Motion Analysis"
  abstract: "Recent advancement of imaging sensors and deep learning techniques has opened door to many interesting applications for visual analysis of human motions. In this talk, I will discuss our research efforts toward addressing the related tasks of 3-D human motion syntheses, pose and shape estimation from images and videos, visual action quality assessment. Looking forward, our results could be applied to everyday life scenarios such as natural user interface, AR/VR, robotics, and gaming, among others."
  bio: "Li Cheng is a professor at the Department of Electrical and Computer Engineering, University of Alberta. He is associate editors of IEEE Trans. Multimedia and Pattern Recognition Journal. Prior to joining University of Alberta, He worked at A*STAR, Singapore, TTI-Chicago, USA, and NICTA, Australia. His current research interests are mainly on human motion analysis, mobile and robot vision, and machine learning. More details can be found at http://www.ece.ualberta.ca/~lcheng5/."
  tags: ["human motion analysis", "visual computing"]

- date: "2023-09-05"
  speaker: "Eriq Augustine"
  speakerUrl: "https://users.soe.ucsc.edu/~eaugusti/"
  speakerPhoto: "eriq_augustine.jpg"
  speakerInfo: "Postdoctoral researcher at UC Santa Cruz"
  video: "https://stream.sfu.ca/Media/Play/8a122362d20e483f857f643297fd1a851d"
  title: "Building Practical Statistical Relational Learning Systems"
  abstract: "In our increasingly connected world, data comes from many different sources, in many different forms, and is noisy, complex, and structured. To confront modern data, we need to embrace the structure inherent in the data and in the predictions. An effective means of approaching this problem of structured prediction, is statistical relational learning (SRL). SRL frameworks use weighted logical and arithmetic expressions to easily create probabilistic graphical models (PGMs) to jointly reason over interdependent data. However, despite being well suited for modern, interconnected data, SRL has several challenges that keep it from becoming practical and widely used in the machine learning community. In this talk, I address four pillars of practicality for SRL systems: scalability, expressivity, model adaptability, and usability."
  bio: "Eriq Augustine (he/him) started his Computer Science career at California State Polytechnic University, San Luis Obispo, where he earned his BS and MS in Computer Science in 2013. Eriq's Master’s research was on micro-text classification and was done under Dr. Alexander Dekhtyar. After his MS, Eriq worked as a software developer on Netflix’s cloud reliability team, where he implemented his MS thesis, SPOONS, on Spanish language tweets to support the South American release of Netflix. Next, Eriq worked as a software developer on the Extensions Engine Team on the Google Chromium/Chrome web browser. There he designed and implemented the Native Messaging API. After his time at Google, Eriq became a senior software developer at Gaine Solutions, where he worked on master data management and production-scale entity resolution systems. Finally, Eriq returned to academia to pursue a Ph.D. in machine learning. He entered the graduate program at the University of California, Santa Cruz, where he joined the LINQS Lab under Dr. Lise Getoor. In Winter of 2023, Eriq graduated with a PhD in Computer Science from UCSC and started working as a postdoctoral researcher under Dr. Getoor. Eriq's research interests lies at the intersection of relational information (e.g., logic) and machine learning. He focuses on the building of practical relational learning systems with an emphasis on scalability, expressivity, model adaptability, and usability. His particular focus leans towards understanding the scalability of machine learning systems from the database and systems perspective."
  tags: ["statistical relational learning", "ai"]

- date: "2023-06-26"
  speaker: "Mika Uy"
  speakerUrl: "https://mikacuy.github.io/"
  speakerPhoto: "mika_uy.jpg"
  speakerInfo: "PhD student at Stanford"
  video: "https://stream.sfu.ca/Media/Play/17d663a3fe654ee3a40d88adb85f456e1d"
  title: "Towards Controllable 3D Content Creation by leveraging Geometric Priors"
  abstract: "The growing popularity for extended realities pushes the demand for the automatic creation and synthesis of new 3D content that would otherwise be a tedious and laborious process. A key property needed to make 3D content creation useful is user controllability as it allows one to realize specific ideas. User-control can be of various forms, e.g. target scans, input images or programmatic edits etc. In this talk, I will be touching works that enable user-control through i) object parts and ii) sparse scene images by leveraging geometric priors. The former utilizes object semantic priors by proposing a novel shape space factorization through an introduced cross diffusion network that enabled multiple applications in both shape generation and editing. The latter leverages pretrained models of large 2D datasets for sparse view 3D NeRF reconstruction of scenes by learning a distribution of geometry represented as ambiguity-aware depth estimates. As an add-on, we will also briefly revisit the volume rendering equation in NeRFs and reformulate it to piecewise linear density that alleviates underlying issues caused by quadrature instability."
  bio: "Mika is a fourth year PhD student at Stanford advised by Leo Guibas. Her research focuses on the representation and generation of objects/scenes for user-controllable 3D content creation. She was a research intern at Adobe, Autodesk and now, Google, and is generously supported by Apple AI/ML PhD Fellowship and Snap Research Fellowship."
  tags: ["3d content creation", "visual computing"]

- date: "2023-06-02"
  speaker: "Silvia Sellan"
  speakerUrl: "https://www.silviasellan.com/"
  speakerPhoto: "silvia_sellan.jpg"
  speakerInfo: "PhD student at Univeristy of Toronto"
  video: "https://stream.sfu.ca/Media/Play/10fc4229c5cf43e8b7f2efa29689cd241d"
  title: "Uncertain Surface Reconstruction"
  abstract: "We propose a method to introduce uncertainty to the surface reconstruction problem. Specifically, we introduce a statistical extension of the classic Poisson Surface Reconstruction algorithm for recovering shapes from 3D point clouds. Instead of outputting an implicit function, we represent the reconstructed shape as a modified Gaussian Process, which allows us to conduct statistical queries (e.g., the likelihood of a point in space being on the surface or inside a solid). We show that this perspective improves PSR's integration into the online scanning process, broadens its application realm, and opens the door to other lines of research such as applying task-specific priors."
  bio: "Silvia is a fourth year Computer Science PhD student at the University of Toronto. She is advised by Alec Jacobson and working in Computer Graphics and Geometry Processing. She is a Vanier Doctoral Scholar, an Adobe Research Fellow and the winner of the 2021 University of Toronto Arts & Science Dean’s Doctoral Excellence Scholarship. She has interned twice at Adobe Research and twice at the Fields Institute of Mathematics. She is also a founder and organizer of the Toronto Geometry Colloquium and a member of WiGRAPH. She is currently looking to survey potential future postdoc and faculty positions, starting Fall 2024."
  tags: ["geometry", "surface reconstruction", "visual computing"]

- date: "2023-05-26"
  speaker: "Daniel Weiskopf"
  speakerUrl: "https://www.vis.uni-stuttgart.de/en/team/Weiskopf/"
  speakerPhoto: "daniel_weiskopf.jpg"
  speakerInfo: "Professor at Univeristy of Stuttgart"
  video: "https://stream.sfu.ca/Media/Play/a0b66c0d023f48f5bbe1d7790fa1b8681d"
  title: "Multidimensional Visualization"
  abstract: "Multidimensional data analysis is of broad interest for a wide range of applications. In this talk, I discuss visualization approaches that support the analysis of such data. I start with a brief overview of the field, a conceptual model, and a discussion of visualization strategies. This part is accompanied by a few examples of recent advancements, with a focus on results from my own work. In the second part, I detail techniques that enrich basic visual mappings like scatterplots, parallel coordinates, or plots of dimensionality reduction by incorporating local correlation analysis. I also discuss sampling issues in multidimensional visualization, and how we can extend it to uncertainty visualization. The talk closes with an outlook on future research directions."
  bio: "Daniel Weiskopf is a professor and one of the directors of the Visualization Research Center (VISUS) and acting director of the Institute for Visualization and Interactive Systems (VIS), both at the University of Stuttgart, Germany. He received his Dr. rer. nat. (PhD) degree in physics from the University of Tübingen, Germany (2001), and the Habilitation degree in computer science at the University of Stuttgart, Germany (2005). His research interests include visualization, visual analytics, eye tracking, human-computer interaction, computer graphics, augmented and virtual reality, and special and general relativity. He is spokesperson of the DFG-funded Collaborative Research Center SFB/Transregio 161 “Quantitative Methods for Visual Computing” (www.sfbtrr161.de), which covers basic research on visualization, including multidimensional visualization."
  tags: ["visualization"]

- date: "2023-04-28"
  speaker: "Karsten Kreis"
  speakerUrl: "https://karstenkreis.github.io/"
  speakerPhoto: "karsten_kreis.png"
  speakerInfo: "Senior research scientist at NVIDIA"
  video: "https://stream.sfu.ca/Media/Play/511bddfec7114912a8adebd1c3acc2281d"
  title: "Diffusion Models: From Foundations to Image, Video and 3D Content Creation"
  abstract: "Denoising diffusion-based generative models have led to multiple breakthroughs in deep generative learning. In this talk, I will provide an overview over recent works by the NVIDIA Toronto AI Lab on diffusion models and their applications for digital content creation. I will start with a short introduction of diffusion models and recapitulate their mathematical formulation. Then, I will briefly discuss our foundational works on diffusion models, which includes advanced diffusion processes for faster and smoother diffusion and denoising, techniques for more efficient model sampling, as well as latent space diffusion models, a flexible diffusion model framework that has been widely used in the literature. Moreover, I will discuss works that use diffusion models for image, video and 3D content creation. This includes large text-to-image models as well as recent work on high resolution video synthesis with latent diffusion models. I will also summarize some of our efforts on 3D generative modeling. This includes object-centric 3D synthesis by training diffusion models on geometric shape datasets or leveraging large-scale text-to-image diffusion models as priors for shape distillation, as well as full scene-level generation with hierarchical latent diffusion models."
  bio: "Karsten Kreis is a senior research scientist at NVIDIA’s Toronto AI Lab. Prior to joining NVIDIA, he worked on deep generative modeling at D-Wave Systems and co-founded Variational AI, a startup utilizing generative models for drug discovery. Before switching to deep learning, Karsten did his M.Sc. in quantum information theory at the Max Planck Institute for the Science of Light and his Ph.D. in computational and statistical physics at the Max Planck Institute for Polymer Research. Currently, Karsten’s research focuses on developing novel generative learning methods and on applying deep generative models on problems in areas such as computer vision, graphics and digital artistry, as well as in the natural sciences."
  tags: ["generative models","diffusion","ai"]

- date: "2023-03-31"
  speaker: "Vered Shwartz"
  speakerUrl: "https://www.cs.ubc.ca/~vshwartz/"
  speakerPhoto: "vered_shwartz.jpg"
  speakerInfo: "Assistant Professor at UBC, Computer Science"
  video: ""
  title: "Incorporating Commonsense Reasoning into NLP Models"
  abstract: "Human language is often ambiguous, underspecified, and grounded in the physical world and in social norms. As humans, we employ commonsense knowledge and reasoning abilities to fill in those gaps and understand others. Endowing NLP models with the same abilities is imperative for reaching human-level language understanding and generation skills. In this talk, I will present several lines of work in which we test NLP models on their commonsense reasoning abilities, develop commonsense reasoning models, and incorporate them into models to improve the performance on NLP tasks."
  bio: "Vered Shwartz is an Assistant Professor of Computer Science at the University of British Columbia, and a CIFAR AI Chair at the Vector Institute. Her research interests include commonsense reasoning, computational semantics and pragmatics, and multiword expressions. Previously, Vered was a postdoctoral researcher at the Allen Institute for AI (AI2) and the University of Washington, and received her PhD in Computer Science from Bar-Ilan University. Vered's work has been recognized with several awards, including The Eric and Wendy Schmidt Postdoctoral Award for Women in Mathematical and Computing Sciences, the Clore Foundation Scholarship, and an ACL 2016 outstanding paper award."
  tags: ["commonsense reasoning", "nlp", "ai"]

- date: "2023-03-17"
  speaker: "Paul Oh"
  speakerUrl: "https://www.unlv.edu/people/paul-oh"
  speakerPhoto: "paul_oh.jpg"
  speakerInfo: "Professor at University of Nevada, Las Vegas (UNLV)"
  video: "https://stream.sfu.ca/Media/Play/141ac6a64aa146d5b7fe13a073cec6e71d"
  title: "From Disaster Response to Consumer Robotics"
  abstract: "The lines between consumer electronics and consumer robotics is blurry. For example, at the annual Consumer Electronics Show (CES) in Las Vegas, the list of robotics companies exhibits has grown to over 400. Furthermore driverless cars, drones, exo-skeletons, 3D printers and virtual-reality systems are examples of robots that have a consumer focus. This talk highlights observations of this phenomena. This is given in the context of an Age of Acceleration characterized by deep learning, cloud-computing, and artificial intelligence. The talk serves to suggest pathways for roboticists and their design and development endeavors."
  bio: "Prof. Paul Oh joined the University of Nevada, Las Vegas (UNLV) as the Lincy Professor of Unmanned Aerial Systems in 2014. He is the founder and director of the Drones and Autonomous Systems Lab (DASL). Prior, he was in Drexel University's Mechanical Engineering Department from 2000-2014. He received mechanical engineering degrees from McGill (B. Eng 1989), Seoul National (M. Sc. 1992), and Columbia (PhD 1999) universities. He is a Fellow of NASA (2002), Naval Research Lab (2003), Boeing (2006) and ASME (2012). He received research (2004 NSF CAREER) and teaching (2005 SAE Ralph Teetor Award for Engineering Education Excellence) awards and authored over 150 publications and 3 books. From 2008-2010, he served as an NSF Program Director managing the robotics research portfolio. He has lead Teams DRC-Hubo, DRC-Hubo@UNLV and Avatar-Hubo for the 2012-2014, 2015, and 2018-2022 DARPA Robotics Challenges Semi-Finals, Finals, and Avatar XPrize respectively. He recently served as General Chair for IEEE IROS 2020 (IEEE Intelligent Robots and Systems) Conference which gathered over 25,000 online attendees."
  tags: ["robotics"]

- date: "2022-11-04"
  speaker: "Derek Liu"
  speakerUrl: "https://www.dgp.toronto.edu/~hsuehtil/"
  speakerPhoto: "derek_liu.jpg"
  speakerInfo: "Research Scientist at Roblox"
  video: ""
  title: "Generative Models for Stylized Geometry"
  abstract: "Recent advances in stylizing 2D digital content have sparked a plethora of image stylization and non-photorealistic rendering technologies. However, how to generate stylized 3D geometry remains a challenging problem. One major reason is the lack of suitable “languages” for computers to understand the style of 3D objects. In this talk, I will cover three increasingly popular perspectives for computers to capture geometric styles, including rendering, machine-learned geometric prior, and surface normals. I will demonstrate how these perspectives can enable computers to generate stylized 3D content. I argue that exploring fundamental style elements for geometry would unlock the door to learning-based and optimization-based techniques for geometric stylization."
  bio: "Hsueh-Ti Derek Liu is a Research Scientist at Roblox working on digital geometry processing and 3D machine learning, based out of Vancouver. Derek’s work mainly focuses on developing easy-to-use 3D modeling tools and numerical methods for processing geometric data at scale. He obtained his PhD at the University of Toronto advised by Prof. Alec Jacobson. He worked as a visiting scholar at École Polytechnique in 2019, working with Prof. Maks Ovsjanikov. He completed his M.S. with Profs. Keenan Crane and Levent Burak Kara at Carnegie Mellon University."
  tags: ["generative models", "geometry", "3d content generation", "visual computing"]