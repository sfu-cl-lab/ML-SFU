webpackJsonp([1],{"+ooZ":function(e,a){},"03Yy":function(e,a,t){e.exports=t.p+"static/img/puzzlefusion.d904f6d.png"},"0Csf":function(e,a){},"19Oi":function(e,a,t){e.exports=t.p+"static/img/MartinEster.156165e.jpg"},"2XjR":function(e,a,t){e.exports=t.p+"static/img/JasonPeng.1f7b44e.jpg"},"2qm3":function(e,a,t){e.exports=t.p+"static/img/neuralgraphgen.5da2a22.png"},"3DWN":function(e,a,t){e.exports=t.p+"static/img/GregMori.bab1c7e.jpg"},"4O0R":function(e,a){},"6gE5":function(e,a,t){e.exports=t.p+"static/img/nerfrevisited.3aef1a3.png"},"8Csy":function(e,a){e.exports="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA1NzYgNTEyIj48cGF0aCAgZmlsbD0iI2VlZSIgZD0iTTU3NiAyNHYxMjcuOTg0YzAgMjEuNDYxLTI1Ljk2IDMxLjk4LTQwLjk3MSAxNi45NzFsLTM1LjcwNy0zNS43MDktMjQzLjUyMyAyNDMuNTIzYy05LjM3MyA5LjM3My0yNC41NjggOS4zNzMtMzMuOTQxIDBsLTIyLjYyNy0yMi42MjdjLTkuMzczLTkuMzczLTkuMzczLTI0LjU2OSAwLTMzLjk0MUw0NDIuNzU2IDc2LjY3NmwtMzUuNzAzLTM1LjcwNUMzOTEuOTgyIDI1LjkgNDAyLjY1NiAwIDQyNC4wMjQgMEg1NTJjMTMuMjU1IDAgMjQgMTAuNzQ1IDI0IDI0ek00MDcuMDI5IDI3MC43OTRsLTE2IDE2QTIzLjk5OSAyMy45OTkgMCAwIDAgMzg0IDMwMy43NjVWNDQ4SDY0VjEyOGgyNjRhMjQuMDAzIDI0LjAwMyAwIDAgMCAxNi45Ny03LjAyOWwxNi0xNkMzNzYuMDg5IDg5Ljg1MSAzNjUuMzgxIDY0IDM0NCA2NEg0OEMyMS40OSA2NCAwIDg1LjQ5IDAgMTEydjM1MmMwIDI2LjUxIDIxLjQ5IDQ4IDQ4IDQ4aDM1MmMyNi41MSAwIDQ4LTIxLjQ5IDQ4LTQ4VjI4Ny43NjRjMC0yMS4zODItMjUuODUyLTMyLjA5LTQwLjk3MS0xNi45N3oiLz48L3N2Zz4="},"8uyo":function(e,a,t){e.exports=t.p+"static/img/AngelChang.9cfd4e9.jpg"},"9wbl":function(e,a){},A3ao:function(e,a,t){e.exports=t.p+"static/img/AngelicaLim.8f35a4e.jpg"},Ae9R:function(e,a,t){e.exports=t.p+"static/img/large6.91cd944.jpg"},B6ej:function(e,a,t){e.exports=t.p+"static/img/large3.474c3d8.jpg"},CWQC:function(e,a,t){e.exports=t.p+"static/img/hci.036e767.jpg"},Ciri:function(e,a,t){e.exports=t.p+"static/img/d2csg.d4cc27a.png"},EeCF:function(e,a,t){e.exports=t.p+"static/img/large4.026a7bd.jpg"},EhV0:function(e,a,t){e.exports=t.p+"static/img/slime.9289a9f.gif"},"G/dI":function(e,a,t){e.exports=t.p+"static/img/large1.fa212a9.jpg"},Gram:function(e,a,t){var i={"./Hamarneh.":"hmPG","./Hamarneh.jpg":"hmPG","./Hamarneh_old.":"Wtrs","./Hamarneh_old.jpg":"Wtrs","./autonomy.":"rWNM","./autonomy.jpg":"rWNM","./backup-datamining.":"To4p","./backup-datamining.jpg":"To4p","./backup-natlang.":"N6L6","./backup-natlang.jpg":"N6L6","./cl.":"I7+U","./cl.jpg":"I7+U","./datamining.":"UbYJ","./datamining.jpg":"UbYJ","./gruvi-family-2014.":"MdgM","./gruvi-family-2014.jpg":"MdgM","./hci.":"CWQC","./hci.jpg":"CWQC","./lab.yaml":"+ooZ","./natlang.":"M9wg","./natlang.jpg":"M9wg","./vml.":"bNiM","./vml.jpg":"bNiM"};function n(e){return t(r(e))}function r(e){var a=i[e];if(!(a+1))throw new Error("Cannot find module '"+e+"'.");return a}n.keys=function(){return Object.keys(i)},n.resolve=r,e.exports=n,n.id="Gram"},I23i:function(e,a,t){e.exports=t.p+"static/img/viper.a9076ae.png"},"I7+U":function(e,a,t){e.exports=t.p+"static/img/cl.8f7452d.jpg"},JgfQ:function(e,a){e.exports="data:image/png;base64,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"},L5Hb:function(e,a){},M9wg:function(e,a,t){e.exports=t.p+"static/img/natlang.331bcc8.jpg"},MFhi:function(e,a,t){e.exports=t.p+"static/img/WuyangChen.3cc3619.jpg"},MIwB:function(e,a){},MPzD:function(e,a){},MUvA:function(e,a,t){e.exports=t.p+"static/img/ManolisSavva.82ccf14.jpg"},MdgM:function(e,a,t){e.exports=t.p+"static/img/gruvi-family-2014.4bd22a5.jpg"},MhgS:function(e,a,t){e.exports=t.p+"static/img/AndreaTagliasacchi.bb44624.png"},MkOt:function(e,a,t){e.exports=t.p+"static/img/neuralfields.325bc1b.jpg"},MlZC:function(e,a,t){var i={"./2023/neurips/bioscan1m.":"qmtF","./2023/neurips/bioscan1m.png":"qmtF","./2023/neurips/d2csg.":"Ciri","./2023/neurips/d2csg.png":"Ciri","./2023/neurips/divinet.":"f16l","./2023/neurips/divinet.png":"f16l","./2023/neurips/mvdiffusion.":"rgWR","./2023/neurips/mvdiffusion.png":"rgWR","./2023/neurips/nerfrevisited.":"6gE5","./2023/neurips/nerfrevisited.png":"6gE5","./2023/neurips/neuralfields.":"MkOt","./2023/neurips/neuralfields.jpg":"MkOt","./2023/neurips/neuralgraphgen.":"2qm3","./2023/neurips/neuralgraphgen.png":"2qm3","./2023/neurips/papr.":"UXfE","./2023/neurips/papr.png":"UXfE","./2023/neurips/polydiffuse.":"yz7y","./2023/neurips/polydiffuse.png":"yz7y","./2023/neurips/puzzlefusion.":"03Yy","./2023/neurips/puzzlefusion.png":"03Yy","./2023/neurips/viper.":"I23i","./2023/neurips/viper.png":"I23i","./2024/iclr/iceformer.":"PJ7a","./2024/iclr/iceformer.png":"PJ7a","./2024/iclr/slime.gif":"EhV0","./2024/iclr/trajeglish.":"srT8","./2024/iclr/trajeglish.png":"srT8","./news.yaml":"YmOC","./pubs.yaml":"aKZY"};function n(e){return t(r(e))}function r(e){var a=i[e];if(!(a+1))throw new Error("Cannot find module '"+e+"'.");return a}n.keys=function(){return Object.keys(i)},n.resolve=r,e.exports=n,n.id="MlZC"},N6L6:function(e,a,t){e.exports=t.p+"static/img/backup-natlang.8fa8046.jpg"},NHnr:function(e,a,t){"use strict";Object.defineProperty(a,"__esModule",{value:!0});var i=t("7+uW"),n=t("ZTUR"),r=t.n(n),s={name:"App",data:function(){return{activeIndex:1,labs:r.a.labs,people:r.a.people,seminars:r.a.seminars,general:r.a.general}},created:function(){},methods:{}},o={render:function(){var e=this,a=e.$createElement,i=e._self._c||a;return i("div",{attrs:{id:"app"}},[i("header",{staticClass:"header"},[e._m(0),e._v(" "),i("div",{staticClass:"header-title"},[i("div",[i("div",[e._v(e._s(e.general.sub_title))]),e._v(" "),i("div",{staticStyle:{"font-size":"1.3em","font-weight":"700",color:"#444"}},[e._v(e._s(e.general.main_title))])])])]),e._v(" "),i("div",{staticClass:"header-divider"}),e._v(" "),i("el-container",{staticStyle:{"overflow-x":"hidden"}},[i("el-aside",{staticClass:"menu-col",attrs:{width:"20%"}},[i("section",{staticClass:"menu"},[i("el-menu",{attrs:{"default-openeds":[],"default-active":"1","background-color":"#2b2925","text-color":"#fff","active-text-color":"#ffd04b"}},[i("a",{attrs:{href:"../#/home"}},[i("el-menu-item",{attrs:{index:"home",router:"true"}},[i("template",{slot:"title"},[i("span",[e._v("Home")])])],2)],1),e._v(" "),i("a",{attrs:{href:"../#/seminars"}},[i("el-menu-item",{attrs:{index:"seminars",router:"true"}},[i("template",{slot:"title"},[i("span",[e._v("Seminars")])])],2)],1),e._v(" "),i("a",{attrs:{href:"../#/news"}},[i("el-menu-item",{attrs:{index:"news",router:"true"}},[i("template",{slot:"title"},[i("span",[e._v("News")])])],2)],1),e._v(" "),i("a",{attrs:{href:"../#/pubs"}},[i("el-menu-item",{attrs:{index:"publications",router:"true"}},[i("template",{slot:"title"},[i("span",[e._v("Publications")])])],2)],1),e._v(" "),i("el-submenu",{attrs:{index:"1"}},[i("template",{slot:"title"},[i("span",[e._v("People")])]),e._v(" "),e._l(e.people,function(a,n){return i("a",{key:"people"+n,attrs:{href:a.url,target:"_blank"}},[i("el-menu-item",{attrs:{index:"people"+n}},[e._v(e._s(a.name)+"\n                "),i("img",{staticStyle:{height:"20%","margin-left":"0.2em"},attrs:{src:t("8Csy")}})])],1)})],2),e._v(" "),i("el-submenu",{attrs:{index:"2"}},[i("template",{slot:"title"},[i("span",[e._v("Related Labs")])]),e._v(" "),e._l(e.labs,function(a,n){return i("a",{key:"lab"+n,attrs:{title:"",href:a.url,target:"_blank"}},[i("el-menu-item",{attrs:{index:"lab-"+n}},[e._v(" "+e._s(a.labName)+"\n                "),i("img",{staticStyle:{height:"20%","margin-left":"0.2em"},attrs:{src:t("8Csy")}})])],1)})],2)],1)],1)]),e._v(" "),i("el-aside",{attrs:{width:"80%"}},[i("router-view")],1)],1)],1)},staticRenderFns:[function(){var e=this.$createElement,a=this._self._c||e;return a("div",[a("img",{attrs:{src:t("JgfQ")}})])}]};var l=t("VU/8")(s,o,!1,function(e){t("YZGu")},null,null).exports,c=t("/ocq"),p={render:function(){var e=this,a=e.$createElement,i=e._self._c||a;return i("article",[i("div",{staticClass:"img-wrapper"},[i("img",{attrs:{src:t("Pb4u")("./"+e.profConf.picPath)}})]),e._v(" "),i("h3",{staticClass:"prof-name"},[i("a",{attrs:{target:"_blank",href:e.profConf.url}},[e._v(e._s(e.profConf.name))])]),e._v(" "),i("p",{staticClass:"prof-desc",domProps:{innerHTML:e._s(e.profConf.description)}}),e._v(" "),i("div",{staticStyle:{"margin-top":"1em"}},[e.profConf.url?i("a",{attrs:{target:"_blank",href:e.profConf.url}},[i("el-button",{attrs:{size:"small"}},[e._v("\n        Home Page\n      ")])],1):e._e(),e._v(" "),e.profConf.labUrl?i("a",{attrs:{target:"_blank",href:e.profConf.labUrl}},[i("el-button",{attrs:{size:"small"}},[e._v("\n        Lab Page\n      ")])],1):e._e()])])},staticRenderFns:[]};var h={render:function(){var e=this,a=e.$createElement,i=e._self._c||a;return i("article",[i("div",{staticClass:"image-container"},[i("img",{attrs:{src:t("Gram")("./"+e.labConf.picPath)}})]),e._v(" "),i("a",{attrs:{target:"_blank",href:e.labConf.url}},[i("h3",[e._v(e._s(e._f("toUpper")(e.labConf.labName)))])]),e._v(" "),i("p",{staticClass:"lab-desc"},[e._v(e._s(e.labConf.description))]),e._v(" "),i("a",{attrs:{target:"_blank",href:e.labConf.url}},[i("el-button",[e._v("\n      MORE DETAILS\n    ")])],1)])},staticRenderFns:[]};var d={name:"home",data:function(){return{labConfs:r.a.labs,carouselConfs:r.a.carousel,profConfs:r.a.people,affiliatedConfs:r.a.affiliated,generalConfs:r.a.general}},computed:{},components:{homeprof:t("VU/8")({name:"homelab",data:function(){return{}},props:["profConf"]},p,!1,function(e){t("fkm7")},"data-v-ac6b7154",null).exports,homelab:t("VU/8")({name:"lab",data:function(){return{}},filters:{toUpper:function(e){return e.toUpperCase()}},props:["labConf"]},h,!1,function(e){t("qK1T")},"data-v-a6958980",null).exports},mounted:function(){}},u={render:function(){var e=this,a=e.$createElement,i=e._self._c||a;return i("div",[i("el-carousel",{attrs:{height:"450px"}},e._l(e.carouselConfs,function(e,a){return i("el-carousel-item",{key:a},[i("img",{attrs:{src:t("rR4Y")("./"+e.picPath),height:450}})])})),e._v(" "),i("section",{staticClass:"sayings content-section"},[i("div",[i("h2",{staticClass:"section-title"},[e._v(e._s(e.generalConfs.section_one.name))]),e._v(" "),i("div",{staticStyle:{display:"flex","justify-content":"space-around"}},e._l(e.generalConfs.section_one.cards,function(a,t){return i("el-card",{key:t,staticClass:"why-sfu"},[e._v("\n          "+e._s(a)+"\n        ")])}))]),e._v(" "),e._m(0)]),e._v(" "),i("section",{staticClass:"people content-section"},[e._m(1),e._v(" "),i("div",{staticStyle:{display:"flex","justify-content":"flex-start","flex-wrap":"wrap"}},e._l(e.profConfs,function(e,a){return i("homeprof",{key:a,staticClass:"prof",attrs:{"prof-conf":e}})}))]),e._v(" "),i("section",{staticClass:"people content-section"},[e._m(2),e._v(" "),i("div",{staticStyle:{display:"flex","justify-content":"flex-start","flex-wrap":"wrap"}},e._l(e.affiliatedConfs,function(e,a){return i("homeprof",{key:a,staticClass:"prof",attrs:{"prof-conf":e}})}))]),e._v(" "),i("section",{staticClass:"lab content-section"},[e._m(3),e._v(" "),i("div",{staticStyle:{display:"flex","justify-content":"flex-start","flex-wrap":"wrap"}},e._l(e.labConfs,function(e,a){return i("homelab",{key:a,staticClass:"lab",attrs:{"lab-conf":e}})}))])],1)},staticRenderFns:[function(){var e=this.$createElement,a=this._self._c||e;return a("a",{attrs:{href:"../#/seminars"}},[a("h2",{staticClass:"link-title"},[this._v("VCR/AI Seminars")])])},function(){var e=this.$createElement,a=this._self._c||e;return a("div",[a("h2",{staticClass:"section-title"},[this._v("PEOPLE")])])},function(){var e=this.$createElement,a=this._self._c||e;return a("div",[a("h2",{staticClass:"section-title"},[this._v("AFFILIATED")])])},function(){var e=this.$createElement,a=this._self._c||e;return a("div",[a("h2",{staticClass:"section-title"},[this._v("LABS")])])}]};var g=t("VU/8")(d,u,!1,function(e){t("VlFx")},"data-v-7e539cb0",null).exports,m={render:function(){var e=this,a=e.$createElement,i=e._self._c||a;return i("article",["pubs"===e.item.type?i("h3",{staticClass:"title"},[i("router-link",{attrs:{to:{name:"news-item",params:{id:e.id}}}},[e._v(e._s(e.item.title))])],1):e.item.url?i("h3",{staticClass:"title"},[i("a",{attrs:{href:e.item.url,target:"_blank"}},[e._v(e._s(e.item.title))])]):i("h3",[e._v(e._s(e.item.title))]),e._v(" "),e.item.image?i("div",{staticClass:"img-wrapper"},[i("img",{attrs:{src:t("MlZC")("./"+e.item.image)}})]):e._e(),e._v(" "),e.item.description?i("div",[i("p",[e._v(e._s(e.item.description))])]):e._e()])},staticRenderFns:[]};var f={name:"news",data:function(){return{news:r.a.news}},computed:{},components:{newsitem:t("VU/8")({name:"item",data:function(){return{}},props:["item","id"]},m,!1,function(e){t("9wbl")},"data-v-c5f29e4c",null).exports},mounted:function(){}},b={render:function(){var e=this.$createElement,a=this._self._c||e;return a("div",[a("section",{staticClass:"content-section"},[a("div",[a("h2",{staticClass:"section-title"},[this._v("News")]),this._v(" "),a("div",{staticStyle:{display:"flex","justify-content":"space-around"}},this._l(this.news,function(e,t){return a("newsitem",{key:t,staticClass:"news",attrs:{item:e,id:t}})}))])])])},staticRenderFns:[]};var v=t("VU/8")(f,b,!1,function(e){t("XrH7")},"data-v-ac7560ee",null).exports,w=t("fZjL"),y=t.n(w),A={name:"pub",data:function(){return{}},computed:{first_link:function(){var e=y()(this.pub.links);return this.pub.links[e[0]]}},props:["pub"]},k={render:function(){var e=this,a=e.$createElement,i=e._self._c||a;return i("article",[i("a",{attrs:{target:"_blank",href:e.first_link}},[i("h3",[e._v(e._s(e.pub.title))])]),e._v(" "),i("div",[i("div",[i("b",[e._v("Authors:")]),e._v(" "+e._s(e.pub.authors.join(", ")))]),e._v(" "),i("div",[i("b",[e._v(e._s(e.pub.venue)+" "+e._s(e.pub.year))])])]),e._v(" "),e.pub.workshops?i("div",e._l(e.pub.workshops,function(a,t){return i("a",{key:t,attrs:{href:a.url,target:"_blank"}},[e._v("\n        Workshop on "+e._s(a.name)),i("br")])})):e._e(),e._v(" "),e.pub.image?i("div",{staticClass:"img-wrapper"},[i("a",{attrs:{target:"_blank",href:e.first_link}},[i("img",{attrs:{src:t("MlZC")("./"+e.pub.image)}})])]):e._e(),e._v(" "),i("div",e._l(e.pub.links,function(a,t){return i("a",{key:t,attrs:{target:"_blank",href:a}},[i("el-button",{attrs:{size:"small"}},[e._v(e._s(t))])],1)}))])},staticRenderFns:[]};var I={name:"pubs",data:function(){return{}},components:{pub:t("VU/8")(A,k,!1,function(e){t("o2sx")},"data-v-6747ee6a",null).exports},props:["pubs","title"]},S={render:function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("div",[t("section",{staticClass:"content-section"},[e.title?t("h3",{staticClass:"section-title"},[e._v(e._s(e.title))]):t("h3",{staticClass:"section-title"},[e._v("Publications")]),e._v(" "),t("div",{staticStyle:{display:"flex","justify-content":"center","flex-wrap":"wrap"}},e._l(e.pubs,function(e,a){return t("pub",{key:a,staticClass:"pub",attrs:{pub:e}})}))])])},staticRenderFns:[]};var M=t("VU/8")(I,S,!1,function(e){t("brfo")},"data-v-0d65247c",null).exports,P={name:"news_item_detailed",data:function(){return{}},components:{pubs:M},props:["item"]},C={render:function(){var e=this,a=e.$createElement,i=e._self._c||a;return i("section",{staticClass:"content-section"},[i("h2",{staticClass:"main-title"},[e._v(e._s(e.item.title))]),e._v(" "),e.item.image?i("div",{staticClass:"img-wrapper"},[i("img",{attrs:{src:t("MlZC")("./"+e.item.image)}})]):e._e(),e._v(" "),e.item.description?i("div",[i("p",[e._v(e._s(e.item.description))])]):e._e(),e._v(" "),e.item.main_pubs&&e.item.main_pubs.length?i("div",[i("pubs",{attrs:{pubs:e.item.main_pubs,title:"Main conference papers"}})],1):e._e(),e._v(" "),e.item.workshop_pubs&&e.item.workshop_pubs.length?i("div",[i("pubs",{attrs:{pubs:e.item.workshop_pubs,title:"Workshop papers"}})],1):e._e(),e._v(" "),e.item.other_pubs&&e.item.other_pubs.length?i("div",[i("pubs",{attrs:{pubs:e.item.other_pubs,title:"Other papers"}})],1):e._e()])},staticRenderFns:[]};var j={name:"news_item_page",data:function(){return{news:r.a.news,pubs:r.a.pubs}},computed:{},methods:{getItem:function(e){var a=this.news[e];if(console.log(a),"pubs"===a.type){var t=this.pubs;if(null!=a.year){var i=a.year.toString();t=t.filter(function(e){return e.year.toString()===i})}if(null!=a.venue){var n=a.venue.toLowerCase();t=t.filter(function(e){return e.venue.toLowerCase().startsWith(n)}),a.main_pubs=t.filter(function(e){return e.venue.toLowerCase()===n}),a.workshop_pubs=t.filter(function(e){return e.venue.toLowerCase().endsWith("workshop")}),a.other_pubs=t.filter(function(e){return e.venue.toLowerCase()!==n&&!e.venue.toLowerCase().endsWith("workshop")})}}return a}},components:{news_item_detailed:t("VU/8")(P,C,!1,function(e){t("dVxn")},"data-v-5535e9ac",null).exports},mounted:function(){}},x={render:function(){var e=this.$createElement;return(this._self._c||e)("news_item_detailed",{attrs:{item:this.getItem(this.$route.params.id)}})},staticRenderFns:[]};var L=t("VU/8")(j,x,!1,function(e){t("MIwB")},"data-v-4b06bccc",null).exports,D={name:"pubsall",data:function(){return{pubs:r.a.pubs}},computed:{},methods:{getTitle:function(e,a){var t="Publications";return null!=e&&(t=e+" "+t),null!=a&&(t=a.toUpperCase()+" "+t),t},getFilteredPubs:function(e,a,t){var i=e;if(null!=a){var n=a.toString();i=i.filter(function(e){return e.year.toString()===n})}if(null!=t){var r=t.toLowerCase();i=i.filter(function(e){return e.venue.toLowerCase()===r})}return i}},components:{pubs:M},mounted:function(){}},T={render:function(){var e=this.$createElement;return(this._self._c||e)("pubs",{attrs:{pubs:this.getFilteredPubs(this.pubs,this.$route.params.year,this.$route.params.venue),title:this.getTitle(this.$route.params.year,this.$route.params.venue)}})},staticRenderFns:[]};var U=t("VU/8")(D,T,!1,function(e){t("L5Hb")},"data-v-d4fa957e",null).exports,N={render:function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("article",[t("h3",[e._v(e._s(e.seminar.date)+" "),e.seminar.location?t("span",[e._v("("+e._s(e.seminar.location)+")")]):e._e()]),e._v(" "),t("h3",{staticClass:"speaker-name"},[t("b",[e._v("Speaker")]),e._v(": "),t("a",{attrs:{target:"_blank",href:e.seminar.speakerUrl}},[e._v(e._s(e.seminar.speaker))]),e._v(" "),t("br"),e._v("\n    "+e._s(e.seminar.speakerInfo)+"\n  ")]),e._v(" "),t("div",[e.seminar.title?t("div",[t("b",[e._v("Title:")]),e._v(" "+e._s(e.seminar.title))]):e._e(),e._v(" "),t("br"),e._v(" "),e.seminar.abstract?t("div",{staticClass:"text"},[t("b",[e._v("Abstract:")]),e._v(" "+e._s(e.seminar.abstract))]):e._e(),e._v(" "),t("br"),e._v(" "),e.seminar.bio?t("div",{staticClass:"text"},[t("b",[e._v("Speaker info:")]),e._v(" "+e._s(e.seminar.bio))]):e._e()]),e._v(" "),e.seminar.video?t("div",{staticStyle:{"margin-top":"1em"}},[t("a",{attrs:{target:"_blank",href:e.seminar.video}},[t("el-button",{attrs:{size:"small"}},[e._v("\n        Video\n      ")])],1)]):e._e()])},staticRenderFns:[]};var R=t("VU/8")({name:"seminar",data:function(){return{}},props:["seminar"]},N,!1,function(e){t("4O0R")},"data-v-5d49d7dc",null).exports,z=(new Date).getTime();r.a.seminars.forEach(function(e){e._millisecs=Date.parse(e.date),e._millisecs_daylater=e._millisecs+864e5});var F={name:"seminars",data:function(){return{seminars:r.a.seminars,pastSeminars:r.a.seminars.filter(function(e){return e._millisecs_daylater<=z}),futureSeminars:r.a.seminars.filter(function(e){return e._millisecs_daylater>z}).sort(function(e,a){return e._millisecs-a._millisecs})}},computed:{},components:{seminar:R},mounted:function(){}},H={render:function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("div",[t("section",{staticClass:"content-section"},[e._m(0),e._v(" "),t("h3",{staticClass:"section-title"},[e._v("Upcoming seminars")]),e._v(" "),t("div",{staticStyle:{display:"flex","justify-content":"center","flex-wrap":"wrap"}},e._l(e.futureSeminars,function(e,a){return t("seminar",{key:a,staticClass:"seminar",attrs:{seminar:e}})})),e._v(" "),t("h3",{staticClass:"section-title"},[e._v("Past seminars")]),e._v(" "),t("div",{staticStyle:{display:"flex","justify-content":"center","flex-wrap":"wrap"}},e._l(e.pastSeminars,function(e,a){return t("seminar",{key:a,staticClass:"seminar",attrs:{seminar:e}})}))])])},staticRenderFns:[function(){var e=this.$createElement,a=this._self._c||e;return a("div",[a("h2",{staticClass:"section-title"},[this._v("SFU VCR/AI Seminars")]),this._v(" "),a("div",{staticStyle:{display:"flex","justify-content":"space-around"}},[this._v("\n      We hold a series of seminars where external speakers present about their work in visual computing, robotics, and AI.  "),a("br"),this._v(" Video recordings are made available internally to the SFU community only.\n      ")])])}]};var E=t("VU/8")(F,H,!1,function(e){t("0Csf")},"data-v-18f1a8ac",null).exports;i.default.use(c.a);var B=new c.a({routes:[{path:"/",name:"index",component:g},{path:"/home",name:"home",component:g},{path:"/seminars",name:"seminars",component:E},{path:"/news",name:"news",component:v},{path:"/news/:id",name:"news-item",component:L},{path:"/pubs",name:"pubs",component:U},{path:"/pubs/:year/:venue",name:"pubs-year-venue",component:U,props:!0}]}),W=t("zL8q"),V=t.n(W);t("tvR6"),t("MPzD");t("zzcG"),i.default.use(V.a),i.default.config.productionTip=!1,new i.default({el:"#app",router:B,components:{App:l},template:"<App/>"})},O7nJ:function(e,a,t){e.exports=t.p+"static/img/MoChen.335a7a8.jpg"},OSxg:function(e,a){e.exports="data:image/svg+xml;base64,PHN2ZyBhcmlhLWhpZGRlbj0idHJ1ZSIgZGF0YS1wcmVmaXg9ImZhciIgZGF0YS1pY29uPSJhZGRyZXNzLWJvb2siIGNsYXNzPSJzdmctaW5saW5lLS1mYSBmYS1hZGRyZXNzLWJvb2sgZmEtdy0xNCIgcm9sZT0iaW1nIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48cGF0aCBmaWxsPSJjdXJyZW50Q29sb3IiIGQ9Ik00MzYgMTYwYzYuNiAwIDEyLTUuNCAxMi0xMnYtNDBjMC02LjYtNS40LTEyLTEyLTEyaC0yMFY0OGMwLTI2LjUtMjEuNS00OC00OC00OEg0OEMyMS41IDAgMCAyMS41IDAgNDh2NDE2YzAgMjYuNSAyMS41IDQ4IDQ4IDQ4aDMyMGMyNi41IDAgNDgtMjEuNSA0OC00OHYtNDhoMjBjNi42IDAgMTItNS40IDEyLTEydi00MGMwLTYuNi01LjQtMTItMTItMTJoLTIwdi02NGgyMGM2LjYgMCAxMi01LjQgMTItMTJ2LTQwYzAtNi42LTUuNC0xMi0xMi0xMmgtMjB2LTY0aDIwem0tNjggMzA0SDQ4VjQ4aDMyMHY0MTZ6TTIwOCAyNTZjMzUuMyAwIDY0LTI4LjcgNjQtNjRzLTI4LjctNjQtNjQtNjQtNjQgMjguNy02NCA2NCAyOC43IDY0IDY0IDY0em0tODkuNiAxMjhoMTc5LjJjMTIuNCAwIDIyLjQtOC42IDIyLjQtMTkuMnYtMTkuMmMwLTMxLjgtMzAuMS01Ny42LTY3LjItNTcuNi0xMC44IDAtMTguNyA4LTQ0LjggOC0yNi45IDAtMzMuNC04LTQ0LjgtOC0zNy4xIDAtNjcuMiAyNS44LTY3LjIgNTcuNnYxOS4yYzAgMTAuNiAxMCAxOS4yIDIyLjQgMTkuMnoiPjwvcGF0aD48L3N2Zz4="},PJ7a:function(e,a,t){e.exports=t.p+"static/img/iceformer.79d4c80.png"},Pb4u:function(e,a,t){var i={"./AndreaTagliasacchi.":"MhgS","./AndreaTagliasacchi.png":"MhgS","./AngelChang.":"8uyo","./AngelChang.jpg":"8uyo","./AngelicaLim.":"A3ao","./AngelicaLim.jpg":"A3ao","./AnoopSarkar.":"wmPe","./AnoopSarkar.jpg":"wmPe","./GhassanHamarneh.":"tzTg","./GhassanHamarneh.png":"tzTg","./GregMori.":"3DWN","./GregMori.jpg":"3DWN","./HangMa.":"geGr","./HangMa.jpg":"geGr","./JasonPeng.":"2XjR","./JasonPeng.jpg":"2XjR","./KeLi.":"ZaLy","./KeLi.jpg":"ZaLy","./LinyiLi.":"yt76","./LinyiLi.jpg":"yt76","./ManolisSavva.":"MUvA","./ManolisSavva.jpg":"MUvA","./MartinEster.":"19Oi","./MartinEster.jpg":"19Oi","./MaxwellLibbercht.":"iohU","./MaxwellLibbercht.jpg":"iohU","./MoChen.":"O7nJ","./MoChen.jpg":"O7nJ","./RichardVaughan.":"oabW","./RichardVaughan.jpg":"oabW","./SchulteOliver.":"Ywgb","./SchulteOliver.jpg":"Ywgb","./SharanVaswani.jpeg":"RgsI","./WuyangChen.":"MFhi","./WuyangChen.jpg":"MFhi","./YasuFurukawa.":"XPKb","./YasuFurukawa.jpg":"XPKb","./cjc.":"ciD5","./cjc.jpg":"ciD5","./people.yaml":"dWDO","./someone.svg":"OSxg"};function n(e){return t(r(e))}function r(e){var a=i[e];if(!(a+1))throw new Error("Cannot find module '"+e+"'.");return a}n.keys=function(){return Object.keys(i)},n.resolve=r,e.exports=n,n.id="Pb4u"},RgsI:function(e,a,t){e.exports=t.p+"static/img/SharanVaswani.c72806d.jpeg"},To4p:function(e,a,t){e.exports=t.p+"static/img/backup-datamining.62c912d.jpg"},UXfE:function(e,a,t){e.exports=t.p+"static/img/papr.8c86ffa.png"},UbYJ:function(e,a,t){e.exports=t.p+"static/img/datamining.a2eb49c.jpg"},VlFx:function(e,a){},Wtrs:function(e,a,t){e.exports=t.p+"static/img/Hamarneh_old.2af13fe.jpg"},XPKb:function(e,a,t){e.exports=t.p+"static/img/YasuFurukawa.c5fc21a.jpg"},XrH7:function(e,a){},YZGu:function(e,a){},YmOC:function(e,a){},Ywgb:function(e,a,t){e.exports=t.p+"static/img/SchulteOliver.342c29f.jpg"},ZTUR:function(e,a){e.exports={carousel:[{picPath:"large4.jpg"},{picPath:"large1.jpg"},{picPath:"large2.jpg"},{picPath:"large3.jpg"},{picPath:"large6.jpg"},{picPath:"large5.jpg"}],labs:[{labName:"Robotics Lab",picPath:"autonomy.jpg",description:"The Autonomy Lab's research goal is to increase the capabilities, robustness and overall autonomy of mobile robot systems. Our research has these main themes: applying the latest AI-based sensing techniques to robot navigation and human-robot interaction creating tools and techniques for programming, simulating and evaluating populations of robots applying models of animal behavior to extend or improve robots, particularly multi-robot systems.",url:"http://robotics.sfu.ca/"},{labName:"Natural Language Lab",picPath:"natlang.jpg",description:"Our lab has a particular focus on research into statistical machine translation and the visual and textual summarization of information contained in natural language. We are always looking for motivated, good graduate students. We are also keen on fostering links with industry for joint projects that are based on research grants and/or student internships that allow graduate students to spend time at companies doing high-risk high-reward natural language processing projects.",url:"http://natlang.cs.sfu.ca/"},{labName:"MIA Research Group",picPath:"Hamarneh.jpg",description:"Our team's research focuses on developing novel computer vision and machine learning methods capable of automatically interpreting images to mimic and complement human vision while being faster, more reproducible, and more accurate.",url:"http://www.cs.sfu.ca/~hamarneh"},{labName:"Structured Machine Learning Lab",picPath:"cl.jpg",description:"Machine Learning for Structured Data: complex networks, multi-relational data, event logs. We apply graphical models, deep neural nets, and reinforcement learning.",url:"http://www.sfu.ca/computationallogic.html"},{labName:"Database and Data Mining Lab",picPath:"datamining.jpg",description:"Our lab also investigates emerging applications of information systems posing new database systems challenges such as on-demand map generation and focused web crawlers. The challenges of on-demand map generation, for example, are the complexity of map generalization operations and the ill-defined notion of the quality of the resulting maps. ",url:"http://www.sfu.ca/computing/labs/ddm.html"},{labName:"GrUVi Lab",picPath:"gruvi-family-2014.jpg",description:"We are an inter-disciplinary team of researchers working in visual computing, in particular, computer graphics and computer vision. Current areas of focus include 3D and robotic vision, 3D printing and content creation, animation, AR/VR, geometric and image-based modelling, machine learning, natural phenomenon, and shape analysis.",url:"http://gruvi.cs.sfu.ca/"}],people:[{name:"Oliver Schulte",url:"http://www.cs.sfu.ca/~oschulte/",labUrl:"http://www.sfu.ca/computationallogic.html",picPath:"SchulteOliver.jpg",description:"&#8226; Machine learning for structured data<br> &#8226; Multi-relational and network data<br> &#8226; Sports analytics"},{name:"Martin Ester",url:"http://www.cs.sfu.ca/~ester/",labUrl:"http://www.sfu.ca/computing/labs/ddm.html",picPath:"MartinEster.jpg",description:" &#8226; Data mining and recommendation in social media<br> &#8226; Data mining in biological networks and gene expression data "},{name:"Maxwell Libbrecht",url:"http://www.cs.sfu.ca/~maxwl/",labUrl:"https://www.libbrechtlab.com/",picPath:"MaxwellLibbercht.jpg",description:"&#8226; Machine learning for genomics data"},{name:"Anoop Sarkar",url:"http://www.cs.sfu.ca/~anoop/",labUrl:"http://natlang.cs.sfu.ca/",picPath:"AnoopSarkar.jpg",description:" &#8226; Machine learning for natural language processing<br> &#8226; Statistical machine translation<br> &#8226; Statistical parsing of natural languages "},{name:"Ghassan Hamarneh",url:"http://www.cs.sfu.ca/~hamarneh",labUrl:"http://www.cs.sfu.ca/~hamarneh",picPath:"GhassanHamarneh.png",description:"&#8226; Computer vision and machine learning to mimic, complement, and improve human interpretation of biomedical images"},{name:"Yasutaka Furukawa",url:"http://www.cs.sfu.ca/~furukawa/",labUrl:"http://gruvi.cs.sfu.ca/",picPath:"YasuFurukawa.jpg",description:"&#8226; Computer graphics<br> &#8226; Computer vision"},{name:"Angel Xuan Chang",url:"http://angelxuanchang.github.io/",labUrl:"https://3dlg-hcvc.github.io/",picPath:"AngelChang.jpg",description:" &#8226; Semantics of shapes and scenes<br> &#8226; Representation and acquisition of common sense knowledge<br> &#8226; Reasoning using probabilistic models"},{name:"Manolis Savva",url:"http://msavva.github.io/",labUrl:"https://3dlg-hcvc.github.io/",picPath:"ManolisSavva.jpg",description:"&#8226; Analysis, organization and generation of 3D content"},{name:"Mo Chen",url:"https://sfumars.com/mo-chen/",labUrl:"https://sfumars.com",picPath:"MoChen.jpg",description:"&#8226; Principled Robotic Decision Making"},{name:"Ke Li",url:"http://www.sfu.ca/~keli/",labUrl:"",picPath:"KeLi.jpg",description:"&#8226; Generative Modelling<br> &#8226; Learning to Optimize<br> &#8226; Fast Nearest Neighbour Search"},{name:"Angelica Lim",url:"https://www.sfu.ca/computing/people/faculty/angelicalim.html",labUrl:"https://www.rosielab.ca/",picPath:"AngelicaLim.jpg",description:"&#8226; Human Robot Interaction<br> &#8226; Affective Computing<br> &#8226; Multimodal Perception and Learning<br> &#8226; Developmental Robotics"},{name:"Hang Ma",url:"https://www.cs.sfu.ca/~hangma/",labUrl:"https://robotics.sfu.ca/airob.html",picPath:"HangMa.jpg",description:"&#8226; Artificial Intelligence<br> &#8226; Robotics<br> &#8226; Multi-Agent/Robot Systems<br> &#8226; Machine Learning"},{name:"Andrea Tagliasacchi",url:"https://taiya.github.io/",labUrl:"",picPath:"AndreaTagliasacchi.png",description:"&#8226; Computer Vision<br> &#8226; Computer Graphics<br> &#8226; Geometric Deep Learning<br> &#8226; 3D Perception"},{name:"Sharan Vaswani",url:"https://vaswanis.github.io/",labUrl:"",picPath:"SharanVaswani.jpeg",description:"&#8226; Machine Learning<br> &#8226; Optimization<br> &#8226; Artificial Intelligence"},{name:"Jason Peng",url:"https://xbpeng.github.io/",labUrl:"",picPath:"JasonPeng.jpg",description:"&#8226; Reinforcement Learning<br> &#8226; Computer Animation<br> &#8226; Robotics"},{name:"Wuyang Chen",url:"https://chenwydj.github.io/",labUrl:"https://delta-lab-ai.github.io/index.html",picPath:"WuyangChen.jpg",description:"&#8226; Deep Learning<br> &#8226; Scientific Machine Learning<br> &#8226; Computer Vision"},{name:"Linyi Li",url:"https://linyil.com/",labUrl:"",picPath:"LinyiLi.jpg",description:"&#8226; Deep learning<br> &#8226; Trustworthy machine learning and computer securiy<br> &#8226; Large language models<br> &#8226; Software engineering<br>"}],affiliated:[{name:"Richard Vaughan",url:"https://rtv.github.io/",labUrl:"http://autonomy.cs.sfu.ca/",picPath:"RichardVaughan.jpg",description:"&#8226; Robot navigation and human-robot interaction<br> &#8226; Programming, simulating and evaluating populations of robots<br> &#8226; Applying models of animal behavior to extend or improve multi-robot systems",status:"affiliated"},{name:"Greg Mori",url:"http://www.cs.sfu.ca/~mori/",labUrl:"http://www.cs.sfu.ca/research/groups/VML/index.html",picPath:"GregMori.jpg",description:"&#8226; Computer vision<br> &#8226; Machine learning",status:"affiliated"}],pubs:[{date:"2024-05-06",venue:"ICLR",year:2024,authors:["Yuzhen Mao","Martin Ester","Ke Li"],title:"IceFormer: Accelerated inference with Long-Sequence Transformers on CPUs",image:"2024/iclr/iceformer.png",links:{webpage:"https://yuzhenmao.github.io/IceFormer/",paper:"https://arxiv.org/pdf/2405.02842",code:"https://github.com/yuzhenmao/IceFormer",video:"https://youtu.be/6W0DtYRzFng"}},{date:"2024-05-06",venue:"ICLR",year:2024,authors:["Aliasghar Khani","Saeid Asgari","Aditya Sanghi","Ali Mahdavi Amiri","Ghassan Hamarneh"],title:"SLiMe: Segment Like Me",image:"2024/iclr/slime.gif",links:{paper:"https://arxiv.org/abs/2309.03179",code:"https://github.com/aliasgharkhani/SLiMe",colab:"https://colab.research.google.com/drive/1fpKx6b2hQGEx1GK269vOw_sKeV9Rpnuj?usp=sharing"}},{date:"2024-05-06",venue:"ICLR",year:2024,authors:["Jonah Philion","Xue Bin Peng","Sanja Fidler"],title:"Trajeglish: Traffic Modeling as Next-Token Prediction",image:"2024/iclr/trajeglish.png",links:{webpage:"https://research.nvidia.com/labs/toronto-ai/trajeglish/",paper:"https://arxiv.org/abs/2312.04535",video:"https://youtu.be/Bk1v_Qct3i8"}},{date:"2023-12-16",venue:"NeurIPS workshop",year:2023,authors:["Pablo Millan Arias","Niousha Sadjadi","Monireh Safari","Zeming Gong","Austin T. Wang","Scott C Lowe","Joakim Bruslund Haurum","Iuliia Zarubiieva","Dirk Steinke","Lila Kari","Angel X. Chang","Graham W Taylor"],title:"BarcodeBERT: Transformers for Biodiversity Analysis",links:{paper:"https://arxiv.org/abs/2311.02401",code:"https://github.com/Kari-Genomics-Lab/BarcodeBERT"},workshops:[{name:"Self-Supervised Learning: Theory and Practice",url:"https://sslneurips23.github.io/index.html",shortname:"ssl"}]},{date:"2023-12-16",venue:"NeurIPS workshop",year:2023,authors:["Shichong Peng","Alireza Moazeni","Ke Li"],title:"How Good Are Deep Generative Models for Solving Inverse problems?",links:{webpage:"https://niopeng.github.io/deep-inverse-workshop/",paper:"https://niopeng.github.io/deep-inverse-workshop/contents/paper.pdf",poster:"https://niopeng.github.io/deep-inverse-workshop/contents/poster.pdf"},workshops:[{name:"Deep Learning and Inverse Problems",url:"https://deep-inverse.org/",shortname:"deep-inverse"}]},{date:"2023-12-15",venue:"NeurIPS workshop",year:2023,authors:["Saurabh Mishra","Anant Raj","Sharan Vaswani"],title:"From Inverse Optimization to Feasibility to ERM",links:{paper:"https://arxiv.org/abs/2402.17890"},workshops:[{name:"Optimization for Machine Learning",url:"https://opt-ml.org/",shortname:"opt-ml"}]},{date:"2023-12-15",venue:"NeurIPS workshop",year:2023,authors:["Anh Dang","Reza Babanezhad","Sharan Vaswani"],title:"Noise-adaptive (Accelerated) Stochastic Heavy-Ball Momentum",links:{paper:"https://arxiv.org/abs/2401.06738"},workshops:[{name:"Optimization for Machine Learning",url:"https://opt-ml.org/",shortname:"opt-ml"}]},{date:"2023-12-15",venue:"NeurIPS workshop",year:2023,authors:["Michael Lu","Matin Aghaei","Anant Raj","Sharan Vaswani"],title:"Practical Principled Policy Optimization for Finite MDPs",links:{paper:"https://openreview.net/forum?id=c838OfxQwF"},workshops:[{name:"Optimization for Machine Learning",url:"https://opt-ml.org/",shortname:"opt-ml"}]},{date:"2023-12-15",venue:"NeurIPS workshop",year:2023,authors:["Reza Asad","Reza Babanezhad","Issam Laradji","Nicolas Le Roux","Sharan Vaswani"],title:"Surrogate Minimization: An Optimization Algorithm for Training Large Neural Networks with Model Parallelism",links:{paper:"https://openreview.net/forum?id=AtzAqJQpan"},workshops:[{name:"Optimization for Machine Learning",url:"https://opt-ml.org/",shortname:"opt-ml"}]},{date:"2023-12-15",venue:"NeurIPS workshop",year:2023,authors:["Chen Fan","Sharan Vaswani","Christos Thrampoulidis","Mark Schmidt"],title:"MSL: An Adaptive Momentem-based Stochastic Line-search Framework",links:{paper:"https://openreview.net/forum?id=UfvQbl7Kpx"},workshops:[{name:"Optimization for Machine Learning",url:"https://opt-ml.org/",shortname:"opt-ml"}]},{date:"2023-12-10",venue:"NeurIPS workshop",year:2023,authors:["Tony Shen","Seonghwan Seo","Grayson Lee","Mohit Pandey","Jason R Smith","Artem Cherkasov","Woo Youn Kim","Martin Ester"],title:"TacoGFN: Target-conditioned GFlowNet for Structure-based Drug Design",links:{paper:"https://arxiv.org/abs/2310.03223"},workshops:[{name:"New Frontiers of AI for Drug Discovery and Development",url:"https://ai4d3.github.io/schedule.html",shortname:"ai4d3"},{name:"Generative AI and Biology",url:"https://genbio-workshop.github.io/",shortname:"genbio"}]},{date:"2023-12-10",venue:"NeurIPS Datasets and Benchmarks Track",year:2023,authors:["Zahra Gharaee","Zeming Gong","Nicholas Pellegrino","Iuliia Zarubiieva","Joakim Bruslund Haurum","Scott C Lowe","Jaclyn TA McKeown","Chris CY Ho","Joschka McLeod","Yi-Yun C Wei","Jireh Agda","Sujeevan Ratnasingham","Dirk Steinke","Angel X. Chang","Graham W Taylor","Paul Fieguth"],title:"A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset",image:"2023/neurips/bioscan1m.png",links:{webpage:"https://biodiversitygenomics.net/1M_insects/",paper:"https://arxiv.org/abs/2307.10455"}},{date:"2023-12-10",venue:"NeurIPS",year:2023,authors:["Kiarash Zahirnia","Yaochen Hu","Mark Coates","Oliver Schulte"],title:"Neural Graph Generation from Graph Statistics",image:"2023/neurips/neuralgraphgen.png",links:{paper:"https://openreview.net/pdf?id=EI6BHFKA5p"}},{date:"2023-12-10",venue:"NeurIPS",year:2023,authors:["Yanshu Zhang","Shichong Peng","Seyed Alireza Moazenipourasil","Ke Li"],title:"PAPR: Proximity Attention Point Rendering",image:"2023/neurips/papr.png",links:{webpage:"https://zvict.github.io/papr/",paper:"https://arxiv.org/abs/2307.11086",video:"https://www.youtube.com/watch?v=1atBGH_pDHY"}},{date:"2023-12-10",venue:"NeurIPS",year:2023,authors:["Mikaela Angelina Uy","Kiyohiro Nakayama","Guandao Yang","Rahul Krishna Thomas","Leonidas Guibas","Ke Li"],title:"NeRF Revisited: Fixing Quadrature Instability in Volume Rendering",image:"2023/neurips/nerfrevisited.png",links:{webpage:"https://pl-nerf.github.io/",paper:"https://arxiv.org/abs/2310.20685",code:"https://github.com/mikacuy/PL-NeRF",video:"https://youtu.be/0QGlsQqbDkM"}},{date:"2023-12-10",venue:"NeurIPS",year:2023,authors:["Fenggen Yu","Qimin Chen","Maham Tanveer","Ali Mahdavi Amiri","Hao (Richard) Zhang"],title:"D2CSG: Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts",image:"2023/neurips/d2csg.png",links:{paper:"https://arxiv.org/abs/2301.11497"}},{date:"2023-12-10",venue:"NeurIPS",year:2023,authors:["Aditya Vora","Akshay Gadi Patil","Hao (Richard) Zhang"],title:"DiViNeT: 3D Reconstruction from Disparate Views via Neural Template Regularization",image:"2023/neurips/divinet.png",links:{webpage:"https://aditya-vora.github.io/divinetpp/",paper:"https://arxiv.org/abs/2306.04699"}},{date:"2023-12-10",venue:"NeurIPS",year:2023,authors:["Jiacheng Chen","Ruizhi Deng","Yasutaka Furukawa"],title:"PolyDiffuse: Polygonal Shape Reconstruction via Guided Set Diffusion Model",image:"2023/neurips/polydiffuse.png",links:{webpage:"https://poly-diffuse.github.io/",paper:"https://arxiv.org/abs/2306.01461"}},{date:"2023-12-10",venue:"NeurIPS",year:2023,authors:["Shitao Tang","Fuyang Zhang","Jiacheng Chen","Peng Wang","Yasutaka Furukawa"],title:"MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion",image:"2023/neurips/mvdiffusion.png",links:{paper:"https://arxiv.org/abs/2306.08943"}},{date:"2023-12-10",venue:"NeurIPS",year:2023,authors:["Sepidehsadat Hosseini","Mohammad Amin Shabani","Saghar Irandoust","Yasutaka Furukawa"],title:"PuzzleFusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving",image:"2023/neurips/puzzlefusion.png",links:{webpage:"https://sepidsh.github.io/floorplan_restore/",paper:"https://arxiv.org/pdf/2211.13785v2.pdf"}},{date:"2023-12-10",venue:"NeurIPS",year:2023,authors:["Alejandro Escontrela","Ademi Adeniji","Wilson Yan","Ajay Jain","Xue Bin Peng","Ken Goldberg","Youngwoon Lee","Danijar Hafner","Pieter Abbeel"],title:"Video Prediction Models as Rewards for Reinforcement Learning",image:"2023/neurips/viper.png",links:{webpage:"https://xbpeng.github.io/projects/VIPER/index.html",paper:"https://xbpeng.github.io/projects/VIPER/2023_VIPER.pdf"}},{date:"2023-12-10",venue:"NeurIPS",year:2023,authors:["Fangcheng Zhong","Kyle Fogarty","Param Hanji","Tianhao Wu","Alejandro Sztrajman","Andrew Spielberg","Andrea Tagliasacchi","Petra Bosilj","Cengiz Oztireli"],title:"Neural Fields with Hard Constraints of Arbitrary Differential Order",image:"2023/neurips/neuralfields.jpg",links:{webpage:"https://zfc946.github.io/CNF.github.io",paper:"https://arxiv.org/abs/2306.08943",code:"https://github.com/zfc946/CNF"}},{date:"2023-12-10",venue:"NeurIPS",year:2023,authors:["Eric Hedlin","Gopal Sharma","Shweta Mahajan","Hossam Isack","Abhishek Kar","Andrea Tagliasacchi","Kwang Moo Yi"],title:"Unsupervised Semantic Correspondence Using Stable Diffusion",links:{webpage:"https://ubc-vision.github.io/LDM_correspondences/",paper:"https://arxiv.org/abs/2305.15581",code:"https://github.com/ubc-vision/LDM_correspondences"}}],news:[{date:"2024-05-06",title:"SFU at ICLR 2024",venue:"iclr",year:2024,type:"pubs"},{date:"2023-12-10",title:"SFU at NeurIPS 2023",venue:"neurips",year:2023,type:"pubs"}],seminars:[{date:"2024-09-27",location:"TASC1 9204 12:00pm",speaker:"Konrad Tollmar",speakerUrl:"https://www.kth.se/profile/konrad",speakerInfo:"Research Director at Electronic Arts",title:"We see farther: AI and ML Research at SEED / EA",abstract:"Electronic Arts has always been driven by 'We see farther' - a vision of pushing boundaries and exploring new horizons for computer games. At SEED - a pioneering group within Electronic Arts - we combine creativity with applied research. In this talk, I'll share how we get games smarter â€” by creating characters that feel more alive, generating new worlds on the fly, and making sure the game responds in interesting ways to how players interact with it. You'll get a look at how we're pushing the boundaries of game design and how AI is helping to shape the future interactive entertainment.",bio:"Dr. Konrad Tollmar is the Head of Research at SEED Electronic Arts. His research interests include innovative AI for interactive entertainment, computer vision, machine learning, computer graphics, and perception-based human-computer interfaces. Prior to EA, he was an Associate Professor at KTH and headed the Mobile Service Lab, a visiting Professor at TUB Telekom Innovation Laboratories, a faculty member at LU Ingvar Kamprad Design Centre, a Research Scientist at MIT CSAIL, and a Postdoc at MIT AI Lab.",tags:["graphics"]},{date:"2024-09-13",location:"TASC1 9204 12:00pm",speaker:"Torsten Schaub",speakerUrl:"https://www.cs.uni-potsdam.de/~torsten/",speakerInfo:"Professor at University of Potsdam",title:"Routing and Scheduling in Answer Set Programming applied to Multi-Agent Path Finding",abstract:"We present alternative approaches to routing and scheduling in Answer Set Programming (ASP), and explore them in the context of Multi-agent Path Finding. The idea is to capture the flow of time in terms of partial orders rather than time steps attached to actions and fluents. This also abolishes the need for fixed upper bounds on the length of plans. The trade-off for this avoidance is that (parts of) temporal trajectories must be acyclic, since multiple occurrences of the same action or fluent cannot be distinguished anymore. While this approach provides an interesting alternative for modeling routing, it is without alternative for scheduling since fine-grained timings cannot be represented in ASP in a feasible way. This is different for partial orders that can be efficiently handled by external means such as acyclicity and difference constraints. We formally elaborate upon this idea and present several resulting ASP encodings. Finally, we demonstrate their effectiveness via an empirical analysis.",bio:"Torsten Schaub received his diploma and dissertation in informatics in 1990 and 1992, respectively, from the Technical University of Darmstadt, Germany, and his habilitation in informatics in 1995 from the University of Rennes I, France. From 1990 to 1993 he was a research assistant at the Technical University at Darmstadt. From 1993 to 1995, he was a research associate at IRISA/INRIA at Rennes. In 1995 he became University Professor at the University of Angers. Since 1997, he is University Professor for knowledge processing and information systems at the University of Potsdam. From 2014 to 2019, Torsten Schaub held an Inria International Chair at Inria Rennes - Bretagne Atlantique. Torsten Schaub has become a fellow of the European Association for Artificial Intelligence EurAI in 2012. From 2014 to 2019 he served as President of the Association of Logic Programming and was program (co-)chair of LPNMR'09, ICLP'10, ECAI'14, and upcoming KR'25. The research interests of Torsten Schaub range from the theoretic foundations to the practical implementation of reasoning from incomplete, inconsistent, and evolving information. His particular research focus lies on Answer set programming and materializes at potassco.org, the home of the open source project Potassco bundling software for Answer Set Programming developed at the University of Potsdam. Last but not least, Torsten Schaub is managing and scientific director at Potassco Solutions GmbH.",tags:["ai","multi-agent path planning"]},{date:"2024-07-04",location:"TASC1 9204 12:00pm",speaker:"Konrad Schindler",speakerUrl:"https://prs.igp.ethz.ch/group/people/person-detail.schindler.html",speakerInfo:"Professor at ETH Zurich",video:"https://stream.sfu.ca/Media/Play/2ac329a2e737489ba9a31bec0c6746e61d",title:"Towards off-the-shelf monocular depth estimators",abstract:"With enough knowledge about the world we are looking at, a single image is enough to recover the 3D structure of the scene in front of our eye(s) - when we close one eye we still perceive the world in 3D. Computer vision started to explore monocular depth estimation (â€œmonodepthâ€) much later than depth from stereo images (or, equivalently, camera motion). It is only recently that computational methods for monodepth have reached a comparable maturity. In the talk, I will briefly recap the development of monodepth algorithms over the past 20 years, and present our own contributions to the topic. I will argue that we may be reaching the point where monodepth modules become a commodity that we download and deploy off-the-shelf, just like stereo reconstruction.",bio:"Konrad Schindler received a Ph.D. degree from Graz University of Technology (Austria) in 2003. He was a Photogrammetric Engineer in the private industry and held research positions at Graz University of Technology; Monash University (Melbourne, Australia) and ETH ZÃ¼rich (Switzerland). He was appointed Assistant Professor of Image Understanding at TU Darmstadt (Germany) in 2009. Since 2010, he has been a tenured Professor of Photogrammetry and Remote Sensing at ETH ZÃ¼rich. His research interests include computer vision and machine learning, with a focus on applications in 3D modelling and in Earth observation.",tags:["vision","monocular depth estimation"]},{date:"2024-06-28",location:"TASC1 9204 12:00pm",speaker:"Danica Sutherland",speakerUrl:"https://djsutherland.ml",speakerPhoto:!1,speakerInfo:"Assistant professor at UBC, Computer Science",video:"https://stream.sfu.ca/Media/Play/b10ef76e9d9647cfa352f852cc1227001d",title:"Scaling Graph Transformers with Expander Graphs",abstract:"Following their runaway success in modelling natural language and other areas, there has been much recent attention to different variants of Transformers for graph data, which may be more able to handle long-range dependencies than previous variants of graph neural networks. These models generally allow all nodes to attend to all other nodes, however, leading to both computational and statistical challenges. This talk will present two models that help with this problem. Exphormer adds â€œvirtual nodesâ€ as well as augmenting the attention mechanism with an expander graph, whose mathematical characteristics help propagate long-range information at cost only linear in the size of the graph. Incorporating this into the GraphGPS framework realized state-of-the-art performance on several interesting datasets. For relatively-dense input graphs, however, even attention on the original graph structure may be too expensive to run for especially large graphs. We demonstrate that even low-width graph Transformers can provide useful signal as to which attention pathways are important in a large graph, and use this to propose Spexphormer, a model which scales Exphormers to very large graphs in transductive settings.",bio:"Danica Sutherland is an Assistant Professor at UBC Computer Science, and a Canada CIFAR AI Chair at Amii. She previously did a PhD at Carnegie Mellon, a postdoc at University College Londonâ€™s Gatsby unit, and was a research assistant professor at TTI-Chicago. Her research in general focuses on understanding and improving representation learning, the integration of kernel methods with deep learning, statistical learning theory, and methods for statistical testing about probability distributions. Her knowledge of graph neural networks comes mostly from her PhD student Hamed Shirzad (lead author on both papers in this talk), an SFU CS alum.",tags:["ai","graph","transformer"]},{date:"2024-06-07",speaker:"Kailas Vodrahalli",speakerUrl:"https://stanford.edu/~kailasv/",speakerPhoto:!1,speakerInfo:"PhD student at Stanford",video:"https://stream.sfu.ca/Media/Play/b5779cce64cb48f5a93c5ba4f8dff4431d",title:"Optimizing AI Models for Human Use",abstract:"In many applications, machine learning models are deployed for human use (e.g., a diagnostic assistant for clinicians). Over the last 2 years, this setting has become even more widespread with the adoption of large language models (LLMs) as assistants in a wide variety of contexts spanning both professional and non-professional disciplines. In this talk, we explore through a set of case studies how we might incorporate humans in the ML training paradigm. These case studies include (1) applying classical computer vision models in a clinical trial study to assistant patients in taking higher quality medical images in telemedicine, (2) using an empirical model for human behavior to fine-tune a generic classifier to align with human utilization, and (3) analyzing human interaction patterns with image generation models. We also present some initial results that are inspired by the methods of (2) to the large language model setting. Across these studies, we take the position and discuss why it is imperative to treat humans as part of the model optimization process.",bio:"Kailas Vodrahalli is a 5th year Ph.D. student at Stanford working with Prof. James Zou where he is generously supported by an NSF Graduate Fellowship and a Stanford Graduate Fellowship. He is interested in designing end-to-end optimized ML systems, spanning across hardware and software. Most recently, he has focused on this problem in the context of human-AI interaction, where his research addresses how we might design systems that include a human user in the model design and optimization. He has explored these problems in various contexts including (1) LLMs, where he has worked on questions related to user personalization and LLM output interpretability; (2) VLMs, where he has investigated how users interact with VLMs over time; and (3) medicine, where he has worked on developing machine learning-based, primarily computer vision, algorithms for use in healthcare-related problems with an end-goal of deploying solutions for clinical use.",tags:["ai","human-ai interaction"]},{date:"2024-05-10",speaker:"Jun Jin",speakerUrl:"https://www.ece.ualberta.ca/~jjin5/",speakerPhoto:!1,speakerInfo:"Assistant professor at University of Alberta, Electrical and Computer Engineering",video:"https://stream.sfu.ca/Media/Play/29713be176e24189bf10bd142eb6b3891d",title:"Seeking Universal Computing Models for Robotics -- A rethink on geometry, robotic reinforcement learning, and embodied AI.",abstract:"From visual servoing, ProMPs, and DMPs to reinforcement learning and embodied artificial intelligence, weâ€™ve seen how these computational models excel across various levels of problems in robotics -- from task specification and motion planning to sensorimotor control and reasoning. But, is there a universal computing model for robotics that solves all the problems and its capabilities regarding motion dexterity, environment perception and reasoning, and human-robot interaction scale up with the size of a full spectrum of datasets? In this talk, I will share my research journey on multiple attempts to build general-purpose robotic task solvers. I will start from my early efforts of developing geometric task specification methods, to real-world robotic reinforcement learning solutions, and finally, to our recent work on embodied reasoning, which connects multi-modal LLM to robotics. I aim to demonstrate why, despite progress, these solutions have yet to fully meet our expectations, and I invite a lively discussion on these findings.",bio:"Dr. Jun Jin is a newly joined assistant professor in the ECE department at the University of Alberta and a Fellow of the Alberta Machine Intelligence Institute (Amii). His research focuses on robotic reinforcement learning, which intersects with embodied artificial intelligence, the theory of predictive coding, and open-ended learning agents. Dr. Jin completed his PhD in Computing Science at the University of Alberta in 2021. His work on real-world robotic reinforcement learning was a top-3-finalist of the Outstanding Student Paper Award at ICRA 2022, which is the flagship conference of the IEEE Robotics and Automation Society. He was the recipient of the KUKA Innovation Award global top-5 finalist and invited to live demos at Hannover Messe 2018. Prior to his academic career, he accrued eight years of professional experience in the ICT and construction/mining industries. Jun loves exploring the beautiful trail system in the City of Edmonton. He is enthusiastic about camping and hiking in the Rocky Mountains.",tags:["robotics"]},{date:"2024-04-12",speaker:"Renjie Liao",speakerUrl:"https://lrjconan.github.io/",speakerPhoto:!1,speakerInfo:"Assistant professor at UBC, Electrical and Computer Engineering",video:"https://stream.sfu.ca/Media/Play/e8bb96c3ff594411a6f2f923c3241f481d",title:"Graph Neural Networks Meet Spectral Graph Theory: A Case Study",abstract:"In this talk, we explore the synergy between Graph Neural Networks (GNNs) and Spectral Graph Theory, presenting two advancements: 1) Specformer, a novel spectral GNN architecture, and 2) in-distribution (ID) and out-of-distribution (OOD) generalization bounds for GNNs based on spectral graph theory. In Specformer, we improve traditional spectral GNNs by incorporating a set-to-set spectral filter with self-attention mechanisms, achieving expressiveness while maintaining permutation equivariance. Specformer significantly outperforms existing models in synthetic and real-world datasets, demonstrating a better ability to capture complex spectral patterns. Concurrently, we significantly tighten the PAC-Bayes in-distribution generalization bounds for GNNs and provide new out-of-distribution generalization bounds based on spectral graph theory. Our empirical validations confirm that GNNs achieve strong size generalization performance in cases guaranteed by our theory.",bio:"Renjie Liao is an assistant professor in the Department of Electrical and Computer Engineering and an associated member of the Department of Computer Science at the University of British Columbia (UBC). He is a faculty member at the Vector Institute and a Canada CIFAR AI Chair. Before joining UBC, he was a Visiting Faculty Researcher at Google Brain, working with Geoffrey Hinton and David Fleet. He received his Ph.D. in 2021 from the University of Toronto under the supervision of Richard Zemel and Raquel Urtasun. During his Ph.D., he worked as a Senior Research Scientist at Uber Advanced Technologies Group. He is broadly interested in machine learning and its intersection with computer vision, self-driving, healthcare, and other areas, with a focus on probabilistic and geometric deep learning",tags:["graph neural networks","ai"]},{date:"2024-02-23",speaker:"Jiaoyang Li",speakerUrl:"https://jiaoyangli.me/",speakerPhoto:!1,speakerInfo:"Assistant professor at CMU, Robotics Instutitute of School of Computer Science",video:"https://stream.sfu.ca/Media/Play/38eea7a250b644a7a3e55b8fb167d1f21d",title:"Layout Design for Large-Scale Multi-Robot Coordination",abstract:"Today, thousands of robots are navigating autonomously in warehouses, transporting goods from one location to another. While numerous planning algorithms are developed to coordinate robots more efficiently and robustly, warehouse layouts remain largely unchanged â€“ they still adhere to the traditional pattern designed for human workers rather than robots. In this talk, I will share our recent progress in exploring layout design and optimization to enhance large-scale multi-robot coordination. I will first introduce a direct layout design method, followed by a method to optimize layout generators instead of layouts. I will then extend these ideas to virtual layout design, which does not require changes to the physical world that robots navigate and thus has the potential for applications beyond automated warehouses.",bio:"Jiaoyang Li is an assistant professor at the Robotics Institute of CMU School of Computer Science. She received her Ph.D. in computer science from the University of Southern California (USC) in 2022. Her research interests lie in the coordination of large robot teams. Her research received recognition through prestigious paper awards (e.g., best student paper, best demo, and best student paper nomination at ICAPS in 2020, 2021, and 2023, along with the best paper finalist at MRS in 2023) and competition championships (e.g., winners of NeurIPS Flatland Challenge in 2020 and Flatland 3 in 2021, as well as the League of Robot Runners sponsored by Amazon Robotics in 2023). Her Ph.D. dissertation also received the best dissertation awards from ICAPS, AAMAS, and USC in 2023.",tags:["multi-agent path planning","robotics","aaai workshop"]},{date:"2024-02-23",speaker:"Sven Koenig",speakerUrl:"http://idm-lab.org/",speakerPhoto:!1,speakerInfo:"Professor at University of Southern California, Computer Science",video:"https://stream.sfu.ca/Media/Play/720e288f386645ea9faf1d20462abbd21d",title:"Multi-Agent Path Finding and Its Applications",abstract:"The coordination of robots and other agents becomes more and more important for industry. For example, on the order of one thousand robots already navigate autonomously in Amazon fulfillment centers to move inventory pods all the way from their storage locations to the picking stations that need the products they store (and vice versa). Optimal and even some approximately optimal path planning for these robots is NP-hard, yet one must find high-quality collision-free paths for them in real-time. Algorithms for such multi-agent path-finding problems have been studied in robotics and theoretical computer science for a longer time but are insufficient since they are either fast but of insufficient solution quality or of good solution quality but too slow. In this talk, I will discuss different variants of multi-agent path-finding problems, cool ideas for both solving them and executing the resulting plans robustly, and several of their applications. Our research on this topic has been funded by both NSF and Amazon Robotics.",bio:"Sven Koenig is a professor of computer science at the University of Southern California. Most of his current research focuses on planning for single agents (such as robots) or multi-agent systems. Additional information about him can be found on his webpages: idm-lab.org.",tags:["multi-agent path planning","robotics","aaai workshop"]},{date:"2024-02-23",speaker:"Vahid Babaei",speakerUrl:"https://aidam.mpi-inf.mpg.de/?view=people_vahid",speakerPhoto:!1,speakerInfo:"Max Planck Institute for Informatics",video:"https://stream.sfu.ca/Media/Play/720e288f386645ea9faf1d20462abbd21d",title:"Inverse Design with Neural Surrogate Models",abstract:"The digitalization of manufacturing is turning fabrication hardware into computers. As traditional tools, such as computer aided design, manufacturing, and engineering (CAD/CAM/CAE) lag behind this new paradigm, the field of computational fabrication has recently emerged from computer graphics to address this knowledge gap with a computer-science mindset. Computer graphics is extremely powerful in creating content for the virtual world. The connection is therefore a natural one as the digital fabrication hardware is starving for innovative content. In this talk, I will focus on inverse design, a powerful paradigm of content synthesis for digital fabrication, which creates fabricable designs given the desired performances. Specifically, I will discuss a class of inverse design problems that deals with data-driven neural surrogate models. These surrogates learn and replace a forward process, such as a computationally heavy simulation.",bio:"Vahid Babaei leads the AI aided Design and Manufacturing group at the Computer Graphics Department of the Max Planck Institute for Informatics in SaarbrÃ¼cken, Germany. He was a postdoctoral researcher at the Computational Design and Fabrication Group of Computer Science and Artificial Intelligence Laboratory (CSAIL) at MIT. He obtained his PhD in Computer Science from EPFL. Vahid Babaei is the recipient of the 2023 Germany-wide Curious Mind Award in the area of â€˜AI, Digitalization, and Roboticsâ€™, the Hermann Neuhaus Prize of the Max Planck Society, and two postdoctoral fellowships awarded by the Swiss National Science Foundation. He is interested in developing original computer science methods for both engineering design and advanced manufacturing.",tags:["ai","aaai workshop"]},{date:"2024-02-23",speaker:"Levi Lelis",speakerUrl:"https://webdocs.cs.ualberta.ca/~santanad/",speakerPhoto:!1,speakerInfo:"Assistant Professor at University of Alberta",video:"https://stream.sfu.ca/Media/Play/75343ff8725b4f078386a1759da8fff41d",title:"Learning Options by Extracting Programs from Neural Networks",abstract:"In this talk, I argue for a programmatic mindset in reinforcement learning, proposing that agents should generate libraries of programs encoding reusable behaviors. When faced with a new task, the agent learns how to combine existing programs and generate new ones. This approach can be helpful even when policies are encoded in seemingly non-decomposable representations like neural networks. I will show that neural networks with piecewise linear activation functions can be mapped to a program with if-then-else structures. Such a program can then be easily decomposed into sub-programs with the same input type of the original network. In the case of networks encoding policies, each sub-program can be seen as an option---a temporally extended action. All these sub-programs form a library of agent behaviors that can be reused later, in downstream tasks. Considering that even small networks can encode a large number of sub-programs, we select sub-programs that are likely to generalize to unseen tasks. This is achieved through a subset selection procedure that minimizes the Levin loss. Empirical evidence from challenging exploration scenarios in two grid-world domains demonstrates that our methodology can extract helpful programs, thus speeding up the learning process in tasks that are similar and yet distinct from the one used to train the original model.",bio:"Dr. Levi Lelis is an Assistant Professor at the University of Alberta, an Amii Fellow, and a CIFAR AI Chair. Leviâ€™s research is dedicated to the development of principled algorithms to solve combinatorial search problems. These problems are integral to optimizing tasks in various sectors. Leviâ€™s research group is focused on combinatorial search problems arising from the search for programmatic solutions---computer programs written in a domain-specific language encoding problem solutions. Levi believes that the most promising path to creating agents that learn continually, efficiently, and safely is to represent the agentsâ€™ knowledge programmatically. While programmatic representations offer many advantages, including modularity and reusability, they present a significant challenge: the need to search over large, non-differentiable spaces not suited for gradient descent methods. Addressing this challenge is the current focus of Leviâ€™s work.",tags:["ai","aaai workshop"]},{date:"2024-01-26",speaker:"Jamie Shotton",speakerUrl:"https://jamie.shotton.org/",speakerPhoto:!1,speakerInfo:"Wayve",video:"https://stream.sfu.ca/Media/Play/9c11692a8b6141f88f0fcb601bbe0cdd1d",title:"Frontiers in Embodied AI for Autonomous Driving",abstract:"Over the last decade, fundamental advances in AI have driven unprecedented progress across many disciplines and applications. And yet, despite significant progress, autonomous vehicles are still far from mainstream even after billions of dollars of investment. In this talk weâ€™ll explore whatâ€™s been holding progress back, and how by adopting a modern embodied AI approach to the problem, Wayve is finally unlocking the potential of autonomous driving in complex and unstructured urban environments such as central London. Weâ€™ll also explore some of our latest research in multimodal learning to combine the power of large language models with the driving problem (LINGO-1), and in generative world models as learned simulators trained to predict the future conditioned on ego action (GAIA-1).",bio:"Jamie Shotton is a leader in AI research and development, with a track record of incubating transformative new technologies and experiences from early stage research to shipping product. He is Chief Scientist at Wayve, building foundation models for embodied intelligence, such as GAIA and LINGO, to enable safe and adaptable autonomous vehicles. Prior to this he was Partner Director of Science at Microsoft and head of the Mixed Reality & AI Labs where he shipped foundational features including body tracking for Kinect and the hand- and eye-tracking that enable HoloLens 2â€™s instinctual interaction model. He has explored applications of AI in autonomous driving, mixed reality, virtual presence, human-computer interaction, gaming, robotics, and healthcare. He has received multiple Best Paper and Best Demo awards at top-tier academic conferences, and the Longuet-Higgins Prize test-of-time award at CVPR 2021. His work on Kinect was awarded the Royal Academy of Engineeringâ€™s gold medal MacRobert Award in 2011, and he shares Microsoftâ€™s Outstanding Technical Achievement Award for 2012 with the Kinect engineering team. In 2014 he received the PAMI Young Researcher Award, and in 2015 the MIT Technology Review Innovator Under 35 Award. He was awarded the Royal Academy of Engineeringâ€™s Silver Medal in 2020. He was elected a Fellow of the Royal Academy of Engineering in 2021.",tags:["autonomous driving"]},{date:"2024-01-12",speaker:"Issam Laradji",speakerUrl:"https://issamlaradji.github.io/",speakerPhoto:!1,speakerInfo:"ServiceNow Research / UBC",video:"https://stream.sfu.ca/Media/Play/255c6d1694614b11ad0a70141d3472601d",title:"Exploring the Landscape of Large Language Models: Methods, Challenges, and Future Possibilities",abstract:"In this presentation, weâ€™ll start by introducing Large Language Models (LLMs) like BERT and BART, explaining their core architecture and significant impacts across various domains. Weâ€™ll then conduct a thorough survey of the current state-of-the-art, highlighting the latest breakthroughs and practical applications. However, it is equally crucial to acknowledge the inherent limitations of LLMs, including their substantial computational costs, safety concerns related to biases, and factuality issues where they might generate incorrect information. These constraints drive ongoing research trends like prompt engineering and the development of foundational models. Finally, weâ€™ll take a glimpse into the future and discuss potential applications, including the exciting prospect of AI â€œcopilotsâ€ that can assist in complex tasks.",bio:"Issam Laradji is a research scientist who works at ServiceNow Research and also works as an adjunct professor at the University of British Columbia. He focuses on creating artificial intelligence (AI) systems that can automate a lot of difficult tasks without needing much human input. He has studied at McGill University and the University of British Columbia, specializing in areas like understanding and generating human language, analyzing images and videos, and improving how AI systems work. Issam is really enthusiastic about bringing people who are interested in AI together and making AI technology easier for everyone to use. To help with this, he has developed various tools like Haven-AI, which gives people the ability to create their own AI solutions for many different real-world problems and research projects.",tags:["llm","ai"]},{date:"2023-10-27 ",speaker:"Tat-Jun Chin",speakerUrl:"https://researchers.adelaide.edu.au/profile/tat-jun.chin",speakerPhoto:!1,speakerInfo:"Professor at University of Adelaide",video:"https://stream.sfu.ca/Media/Play/81e9a38ce6b240338561d4ecbf1139101d",title:"Quantum Computing for Robust Fitting",abstract:"Many computer vision applications need to recover structure from imperfect measurements of the real world. The task is often solved by robustly fitting a geometric model onto noisy and outlier-contaminated data. However, relatively recent theoretical analyses indicate that many commonly used formulations of robust fitting in computer vision are not amenable to tractable solution and approximation. In this paper, we explore the usage of quantum computers for robust fitting. To do so, we examine the feasibility of two types of quantum computer technologies---universal gate quantum computers and quantum annealers---to solve robust fitting. Novel algorithms that are amenable to the quantum machines have been developed, and experimental results on current noisy intermediate scale quantum computers (NISQ) will be reported. Our work thus proposes one of the first quantum treatments of robust fitting for computer vision.",bio:"Tat-Jun (TJ) Chin is SmartSat CRC Professorial Chair of Sentient Satellites at The University of Adelaide. He received his PhD in Computer Systems Engineering from Monash University in 2007, which was partly supported by the Endeavour Australia-Asia Award, and a Bachelor in Mechatronics Engineering from Universiti Teknologi Malaysia in 2004, where he won the Vice Chancellorâ€™s Award. TJâ€™s research interest lies in computer vision and machine learning for space applications. He has published close to 200 research articles, and has won several awards for his research, including a CVPR award (2015), a BMVC award (2018), Best of ECCV (2018), three DST Awards (2015, 2017, 2021), an IAPR Award (2019) and an RAL Best Paper Award (2021). TJ pioneered the AI4Space Workshop series and is an Associate Editor at the International Journal of Robotics Research (IJRR) and Journal of Mathematical Imaging and Vision (JMIV). He was a Finalist in the Academic of the Year Category at Australian Space Awards 2021.",tags:["quantum computing","visual computing"]},{date:"2023-10-20",speaker:"Leonid Sigal",speakerUrl:"https://www.cs.ubc.ca/~lsigal/",speakerPhoto:!1,speakerInfo:"Professor at UBC, Computer Science",video:"https://stream.sfu.ca/Media/Play/0377be26412347d5934fbf1675821edb1d",title:"Efficient, Less-biased and Creative Visual Learning",abstract:"In this talk I will discuss recent methods from my group that focus on addressing some of the core challenges of current visual and multi-modal cognition, including efficient learning, bias and user-controlled generation. Centering on these larger themes I will talk about a number of strategies (and corresponding papers) that we developed to address these challenges. I will start by discussing transfer learning techniques in the context of a semi-supervised object detection and segmentation, highlighting a model that is applicable to a range of supervision: from zero to a few instance-level samples per novel class. I will then talk about our recent work on building a foundational image representation model by combining two successful strategies of masking and sequential token prediction. I will also discuss some of our work on scene graph generation which, in addition to improving overall performance, allows for scalable inference and ability to control data bias (by trade off major improvements on rare classes for minor declines on most common classes). The talk will end with some of our recent work on generative modeling which focuses on novel-view synthesis and language-conditioned diffusion-based story generation. The core of the latter approach is visual memory that implicitly captures the actor and background context across the generated frames. Sentence-conditioned soft attention over the memories enables effective reference resolution and learns to maintain scene and actor consistency when needed.",bio:"Prof. Leonid Sigal is a Professor at the University of British Columbia (UBC). He was appointed CIFAR AI Chair at the Vector Institute in 2019 and an NSERC Tier 2 Canada Research Chair in Computer Vision and Machine Learning in 2018. Prior to this, he was a Senior Research Scientist, and a group lead, at Disney Research. He completed his Ph.D at Brown University in 2008; received his B.Sc. degrees in Computer Science and Mathematics from Boston University in 1999, his M.A. from Boston University in 1999, and his M.S. from Brown University in 2003. He was a Postdoctoral Researcher at the University of Toronto, between 2007-2009. Leonidâ€™s research interests lie in the areas of computer vision, machine learning, and computer graphics; with the emphasis on approaches for visual and multi-modal representation learning, recognition, understanding and generative modeling. He has won a number of prestigious research awards, including Killam Accelerator Fellowship in 2021 and has published over 100 papers in venues such as CVPR, ICCV, ECCV, NeurIPS, ICLR, and Siggraph.",tags:["visual learning","visual computing"]},{date:"2023-10-13",speaker:"Li Cheng",speakerUrl:"http://www.ece.ualberta.ca/~lcheng5/",speakerPhoto:!1,speakerInfo:"Professor at University of Alberta, Electrical and Computer Engineering",video:"https://stream.sfu.ca/Media/Play/e1bf6d3177ed4f3288ad7ddde49f286a1d",title:"Visual Human Motion Analysis",abstract:"Recent advancement of imaging sensors and deep learning techniques has opened door to many interesting applications for visual analysis of human motions. In this talk, I will discuss our research efforts toward addressing the related tasks of 3-D human motion syntheses, pose and shape estimation from images and videos, visual action quality assessment. Looking forward, our results could be applied to everyday life scenarios such as natural user interface, AR/VR, robotics, and gaming, among others.",bio:"Li Cheng is a professor at the Department of Electrical and Computer Engineering, University of Alberta. He is associate editors of IEEE Trans. Multimedia and Pattern Recognition Journal. Prior to joining University of Alberta, He worked at A*STAR, Singapore, TTI-Chicago, USA, and NICTA, Australia. His current research interests are mainly on human motion analysis, mobile and robot vision, and machine learning. More details can be found at http://www.ece.ualberta.ca/~lcheng5/.",tags:["human motion analysis","visual computing"]},{date:"2023-09-05",speaker:"Eriq Augustine",speakerUrl:"https://users.soe.ucsc.edu/~eaugusti/",speakerPhoto:!1,speakerInfo:"Postdoctoral researcher at UC Santa Cruz",video:"https://stream.sfu.ca/Media/Play/8a122362d20e483f857f643297fd1a851d",title:"Building Practical Statistical Relational Learning Systems",abstract:"In our increasingly connected world, data comes from many different sources, in many different forms, and is noisy, complex, and structured. To confront modern data, we need to embrace the structure inherent in the data and in the predictions. An effective means of approaching this problem of structured prediction, is statistical relational learning (SRL). SRL frameworks use weighted logical and arithmetic expressions to easily create probabilistic graphical models (PGMs) to jointly reason over interdependent data. However, despite being well suited for modern, interconnected data, SRL has several challenges that keep it from becoming practical and widely used in the machine learning community. In this talk, I address four pillars of practicality for SRL systems: scalability, expressivity, model adaptability, and usability.",bio:"Eriq Augustine (he/him) started his Computer Science career at California State Polytechnic University, San Luis Obispo, where he earned his BS and MS in Computer Science in 2013. Eriq's Masterâ€™s research was on micro-text classification and was done under Dr. Alexander Dekhtyar. After his MS, Eriq worked as a software developer on Netflixâ€™s cloud reliability team, where he implemented his MS thesis, SPOONS, on Spanish language tweets to support the South American release of Netflix. Next, Eriq worked as a software developer on the Extensions Engine Team on the Google Chromium/Chrome web browser. There he designed and implemented the Native Messaging API. After his time at Google, Eriq became a senior software developer at Gaine Solutions, where he worked on master data management and production-scale entity resolution systems. Finally, Eriq returned to academia to pursue a Ph.D. in machine learning. He entered the graduate program at the University of California, Santa Cruz, where he joined the LINQS Lab under Dr. Lise Getoor. In Winter of 2023, Eriq graduated with a PhD in Computer Science from UCSC and started working as a postdoctoral researcher under Dr. Getoor. Eriq's research interests lies at the intersection of relational information (e.g., logic) and machine learning. He focuses on the building of practical relational learning systems with an emphasis on scalability, expressivity, model adaptability, and usability. His particular focus leans towards understanding the scalability of machine learning systems from the database and systems perspective.",tags:["statistical relational learning","ai"]},{date:"2023-06-26",speaker:"Mika Uy",speakerUrl:"https://mikacuy.github.io/",speakerPhoto:!1,speakerInfo:"PhD student at Stanford",video:"https://stream.sfu.ca/Media/Play/17d663a3fe654ee3a40d88adb85f456e1d",title:"Towards Controllable 3D Content Creation by leveraging Geometric Priors",abstract:"The growing popularity for extended realities pushes the demand for the automatic creation and synthesis of new 3D content that would otherwise be a tedious and laborious process. A key property needed to make 3D content creation useful is user controllability as it allows one to realize specific ideas. User-control can be of various forms, e.g. target scans, input images or programmatic edits etc. In this talk, I will be touching works that enable user-control through i) object parts and ii) sparse scene images by leveraging geometric priors. The former utilizes object semantic priors by proposing a novel shape space factorization through an introduced cross diffusion network that enabled multiple applications in both shape generation and editing. The latter leverages pretrained models of large 2D datasets for sparse view 3D NeRF reconstruction of scenes by learning a distribution of geometry represented as ambiguity-aware depth estimates. As an add-on, we will also briefly revisit the volume rendering equation in NeRFs and reformulate it to piecewise linear density that alleviates underlying issues caused by quadrature instability.",bio:"Mika is a fourth year PhD student at Stanford advised by Leo Guibas. Her research focuses on the representation and generation of objects/scenes for user-controllable 3D content creation. She was a research intern at Adobe, Autodesk and now, Google, and is generously supported by Apple AI/ML PhD Fellowship and Snap Research Fellowship.",tags:["3d content creation","visual computing"]},{date:"2023-06-02",speaker:"Silvia Sellan",speakerUrl:"https://www.silviasellan.com/",speakerPhoto:!1,speakerInfo:"PhD student at Univeristy of Toronto",video:"https://stream.sfu.ca/Media/Play/10fc4229c5cf43e8b7f2efa29689cd241d",title:"Uncertain Surface Reconstruction",abstract:"We propose a method to introduce uncertainty to the surface reconstruction problem. Specifically, we introduce a statistical extension of the classic Poisson Surface Reconstruction algorithm for recovering shapes from 3D point clouds. Instead of outputting an implicit function, we represent the reconstructed shape as a modified Gaussian Process, which allows us to conduct statistical queries (e.g., the likelihood of a point in space being on the surface or inside a solid). We show that this perspective improves PSR's integration into the online scanning process, broadens its application realm, and opens the door to other lines of research such as applying task-specific priors.",bio:"Silvia is a fourth year Computer Science PhD student at the University of Toronto. She is advised by Alec Jacobson and working in Computer Graphics and Geometry Processing. She is a Vanier Doctoral Scholar, an Adobe Research Fellow and the winner of the 2021 University of Toronto Arts & Science Deanâ€™s Doctoral Excellence Scholarship. She has interned twice at Adobe Research and twice at the Fields Institute of Mathematics. She is also a founder and organizer of the Toronto Geometry Colloquium and a member of WiGRAPH. She is currently looking to survey potential future postdoc and faculty positions, starting Fall 2024.",tags:["geometry","surface reconstruction","visual computing"]},{date:"2023-05-26",speaker:"Daniel Weiskopf",speakerUrl:"https://www.vis.uni-stuttgart.de/en/team/Weiskopf/",speakerPhoto:!1,speakerInfo:"Professor at Univeristy of Stuttgart",video:"https://stream.sfu.ca/Media/Play/a0b66c0d023f48f5bbe1d7790fa1b8681d",title:"Multidimensional Visualization",abstract:"Multidimensional data analysis is of broad interest for a wide range of applications. In this talk, I discuss visualization approaches that support the analysis of such data. I start with a brief overview of the field, a conceptual model, and a discussion of visualization strategies. This part is accompanied by a few examples of recent advancements, with a focus on results from my own work. In the second part, I detail techniques that enrich basic visual mappings like scatterplots, parallel coordinates, or plots of dimensionality reduction by incorporating local correlation analysis. I also discuss sampling issues in multidimensional visualization, and how we can extend it to uncertainty visualization. The talk closes with an outlook on future research directions.",bio:"Daniel Weiskopf is a professor and one of the directors of the Visualization Research Center (VISUS) and acting director of the Institute for Visualization and Interactive Systems (VIS), both at the University of Stuttgart, Germany. He received his Dr. rer. nat. (PhD) degree in physics from the University of TÃ¼bingen, Germany (2001), and the Habilitation degree in computer science at the University of Stuttgart, Germany (2005). His research interests include visualization, visual analytics, eye tracking, human-computer interaction, computer graphics, augmented and virtual reality, and special and general relativity. He is spokesperson of the DFG-funded Collaborative Research Center SFB/Transregio 161 â€œQuantitative Methods for Visual Computingâ€ (www.sfbtrr161.de), which covers basic research on visualization, including multidimensional visualization.",tags:["visualization"]},{date:"2023-04-28",speaker:"Karsten Kreis",speakerUrl:"https://karstenkreis.github.io/",speakerPhoto:!1,speakerInfo:"Senior research scientist at NVIDIA",video:"https://stream.sfu.ca/Media/Play/511bddfec7114912a8adebd1c3acc2281d",title:"Diffusion Models: From Foundations to Image, Video and 3D Content Creation",abstract:"Denoising diffusion-based generative models have led to multiple breakthroughs in deep generative learning. In this talk, I will provide an overview over recent works by the NVIDIA Toronto AI Lab on diffusion models and their applications for digital content creation. I will start with a short introduction of diffusion models and recapitulate their mathematical formulation. Then, I will briefly discuss our foundational works on diffusion models, which includes advanced diffusion processes for faster and smoother diffusion and denoising, techniques for more efficient model sampling, as well as latent space diffusion models, a flexible diffusion model framework that has been widely used in the literature. Moreover, I will discuss works that use diffusion models for image, video and 3D content creation. This includes large text-to-image models as well as recent work on high resolution video synthesis with latent diffusion models. I will also summarize some of our efforts on 3D generative modeling. This includes object-centric 3D synthesis by training diffusion models on geometric shape datasets or leveraging large-scale text-to-image diffusion models as priors for shape distillation, as well as full scene-level generation with hierarchical latent diffusion models.",bio:"Karsten Kreis is a senior research scientist at NVIDIAâ€™s Toronto AI Lab. Prior to joining NVIDIA, he worked on deep generative modeling at D-Wave Systems and co-founded Variational AI, a startup utilizing generative models for drug discovery. Before switching to deep learning, Karsten did his M.Sc. in quantum information theory at the Max Planck Institute for the Science of Light and his Ph.D. in computational and statistical physics at the Max Planck Institute for Polymer Research. Currently, Karstenâ€™s research focuses on developing novel generative learning methods and on applying deep generative models on problems in areas such as computer vision, graphics and digital artistry, as well as in the natural sciences.",tags:["generative models","diffusion","ai"]},{date:"2023-03-31",speaker:"Vered Shwartz",speakerUrl:"https://www.cs.ubc.ca/~vshwartz/",speakerPhoto:!1,speakerInfo:"Assistant Professor at UBC, Computer Science",video:"",title:"Incorporating Commonsense Reasoning into NLP Models",abstract:"Human language is often ambiguous, underspecified, and grounded in the physical world and in social norms. As humans, we employ commonsense knowledge and reasoning abilities to fill in those gaps and understand others. Endowing NLP models with the same abilities is imperative for reaching human-level language understanding and generation skills. In this talk, I will present several lines of work in which we test NLP models on their commonsense reasoning abilities, develop commonsense reasoning models, and incorporate them into models to improve the performance on NLP tasks.",bio:"Vered Shwartz is an Assistant Professor of Computer Science at the University of British Columbia, and a CIFAR AI Chair at the Vector Institute. Her research interests include commonsense reasoning, computational semantics and pragmatics, and multiword expressions. Previously, Vered was a postdoctoral researcher at the Allen Institute for AI (AI2) and the University of Washington, and received her PhD in Computer Science from Bar-Ilan University. Vered's work has been recognized with several awards, including The Eric and Wendy Schmidt Postdoctoral Award for Women in Mathematical and Computing Sciences, the Clore Foundation Scholarship, and an ACL 2016 outstanding paper award.",tags:["commonsense reasoning","nlp","ai"]},{date:"2023-03-17",speaker:"Paul Oh",speakerUrl:"https://www.unlv.edu/people/paul-oh",speakerPhoto:!1,speakerInfo:"Professor at University of Nevada, Las Vegas (UNLV)",video:"https://stream.sfu.ca/Media/Play/141ac6a64aa146d5b7fe13a073cec6e71d",title:"From Disaster Response to Consumer Robotics",abstract:"The lines between consumer electronics and consumer robotics is blurry. For example, at the annual Consumer Electronics Show (CES) in Las Vegas, the list of robotics companies exhibits has grown to over 400. Furthermore driverless cars, drones, exo-skeletons, 3D printers and virtual-reality systems are examples of robots that have a consumer focus. This talk highlights observations of this phenomena. This is given in the context of an Age of Acceleration characterized by deep learning, cloud-computing, and artificial intelligence. The talk serves to suggest pathways for roboticists and their design and development endeavors.",bio:"Prof. Paul Oh joined the University of Nevada, Las Vegas (UNLV) as the Lincy Professor of Unmanned Aerial Systems in 2014. He is the founder and director of the Drones and Autonomous Systems Lab (DASL). Prior, he was in Drexel University's Mechanical Engineering Department from 2000-2014. He received mechanical engineering degrees from McGill (B. Eng 1989), Seoul National (M. Sc. 1992), and Columbia (PhD 1999) universities. He is a Fellow of NASA (2002), Naval Research Lab (2003), Boeing (2006) and ASME (2012). He received research (2004 NSF CAREER) and teaching (2005 SAE Ralph Teetor Award for Engineering Education Excellence) awards and authored over 150 publications and 3 books. From 2008-2010, he served as an NSF Program Director managing the robotics research portfolio. He has lead Teams DRC-Hubo, DRC-Hubo@UNLV and Avatar-Hubo for the 2012-2014, 2015, and 2018-2022 DARPA Robotics Challenges Semi-Finals, Finals, and Avatar XPrize respectively. He recently served as General Chair for IEEE IROS 2020 (IEEE Intelligent Robots and Systems) Conference which gathered over 25,000 online attendees.",tags:["robotics"]},{date:"2022-11-04",speaker:"Derek Liu",speakerUrl:"https://www.dgp.toronto.edu/~hsuehtil/",speakerPhoto:!1,speakerInfo:"Research Scientist at Roblox",video:"",title:"Generative Models for Stylized Geometry",abstract:"Recent advances in stylizing 2D digital content have sparked a plethora of image stylization and non-photorealistic rendering technologies. However, how to generate stylized 3D geometry remains a challenging problem. One major reason is the lack of suitable â€œlanguagesâ€ for computers to understand the style of 3D objects. In this talk, I will cover three increasingly popular perspectives for computers to capture geometric styles, including rendering, machine-learned geometric prior, and surface normals. I will demonstrate how these perspectives can enable computers to generate stylized 3D content. I argue that exploring fundamental style elements for geometry would unlock the door to learning-based and optimization-based techniques for geometric stylization.",bio:"Hsueh-Ti Derek Liu is a Research Scientist at Roblox working on digital geometry processing and 3D machine learning, based out of Vancouver. Derekâ€™s work mainly focuses on developing easy-to-use 3D modeling tools and numerical methods for processing geometric data at scale. He obtained his PhD at the University of Toronto advised by Prof. Alec Jacobson. He worked as a visiting scholar at Ã‰cole Polytechnique in 2019, working with Prof. Maks Ovsjanikov. He completed his M.S. with Profs. Keenan Crane and Levent Burak Kara at Carnegie Mellon University.",tags:["generative models","geometry","3d content generation","visual computing"]}],general:{main_title:"Machine Learning Website",sub_title:"Computer Science",section_one:{name:"WHY SFU",cards:["Over fifteen faculty members do research on machine learning methods and applications: vision, image analysis, robotics, natural language processing.","Graduate courses for introduction and advanced topics such as deep learning and generative models","Faculty-directed labs have state-of-the-art software and hardware, including GPU and database servers.","Compute Canada provides high-performance cluster computing. The Cedar node, housed at SFU, has 150 GPUs."]}}}},ZaLy:function(e,a,t){e.exports=t.p+"static/img/KeLi.6131a35.jpg"},aKZY:function(e,a){},bNiM:function(e,a,t){e.exports=t.p+"static/img/vml.3f6c387.jpg"},brfo:function(e,a){},ciD5:function(e,a,t){e.exports=t.p+"static/img/cjc.91ddfa2.jpg"},dVxn:function(e,a){},dWDO:function(e,a){},f16l:function(e,a,t){e.exports=t.p+"static/img/divinet.89a543b.png"},fkm7:function(e,a){},geGr:function(e,a,t){e.exports=t.p+"static/img/HangMa.ab19823.jpg"},hmPG:function(e,a,t){e.exports=t.p+"static/img/Hamarneh.026a7bd.jpg"},iohU:function(e,a,t){e.exports=t.p+"static/img/MaxwellLibbercht.693e13c.jpg"},lDYO:function(e,a,t){e.exports=t.p+"static/img/large5.0e3c805.jpg"},o2sx:function(e,a){},oabW:function(e,a){e.exports="data:image/jpeg;base64,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"},qK1T:function(e,a){},qmtF:function(e,a,t){e.exports=t.p+"static/img/bioscan1m.fe2550f.png"},rR4Y:function(e,a,t){var i={"./large1.":"G/dI","./large1.jpg":"G/dI","./large2.":"vmUp","./large2.jpg":"vmUp","./large3.":"B6ej","./large3.jpg":"B6ej","./large4.":"EeCF","./large4.jpg":"EeCF","./large5.":"lDYO","./large5.jpg":"lDYO","./large6.":"Ae9R","./large6.jpg":"Ae9R"};function n(e){return t(r(e))}function r(e){var a=i[e];if(!(a+1))throw new Error("Cannot find module '"+e+"'.");return a}n.keys=function(){return Object.keys(i)},n.resolve=r,e.exports=n,n.id="rR4Y"},rWNM:function(e,a,t){e.exports=t.p+"static/img/autonomy.606831c.jpg"},rgWR:function(e,a,t){e.exports=t.p+"static/img/mvdiffusion.f5078b8.png"},srT8:function(e,a,t){e.exports=t.p+"static/img/trajeglish.b8747e7.png"},tvR6:function(e,a){},tzTg:function(e,a,t){e.exports=t.p+"static/img/GhassanHamarneh.ec64bce.png"},vmUp:function(e,a,t){e.exports=t.p+"static/img/large2.8b8e202.jpg"},wmPe:function(e,a,t){e.exports=t.p+"static/img/AnoopSarkar.762c252.jpg"},yt76:function(e,a,t){e.exports=t.p+"static/img/LinyiLi.071802a.jpg"},yz7y:function(e,a,t){e.exports=t.p+"static/img/polydiffuse.55af908.png"},zzcG:function(e,a,t){var i={"./carousel/large1.jpg":"G/dI","./carousel/large2.jpg":"vmUp","./carousel/large3.jpg":"B6ej","./carousel/large4.jpg":"EeCF","./carousel/large5.jpg":"lDYO","./carousel/large6.jpg":"Ae9R","./lab/Hamarneh.jpg":"hmPG","./lab/Hamarneh_old.jpg":"Wtrs","./lab/autonomy.jpg":"rWNM","./lab/backup-datamining.jpg":"To4p","./lab/backup-natlang.jpg":"N6L6","./lab/cl.jpg":"I7+U","./lab/datamining.jpg":"UbYJ","./lab/gruvi-family-2014.jpg":"MdgM","./lab/hci.jpg":"CWQC","./lab/natlang.jpg":"M9wg","./lab/vml.jpg":"bNiM","./people/AndreaTagliasacchi.png":"MhgS","./people/AngelChang.jpg":"8uyo","./people/AngelicaLim.jpg":"A3ao","./people/AnoopSarkar.jpg":"wmPe","./people/GhassanHamarneh.png":"tzTg","./people/GregMori.jpg":"3DWN","./people/HangMa.jpg":"geGr","./people/JasonPeng.jpg":"2XjR","./people/KeLi.jpg":"ZaLy","./people/LinyiLi.jpg":"yt76","./people/ManolisSavva.jpg":"MUvA","./people/MartinEster.jpg":"19Oi","./people/MaxwellLibbercht.jpg":"iohU","./people/MoChen.jpg":"O7nJ","./people/RichardVaughan.jpg":"oabW","./people/SchulteOliver.jpg":"Ywgb","./people/WuyangChen.jpg":"MFhi","./people/YasuFurukawa.jpg":"XPKb","./people/cjc.jpg":"ciD5","./research/2023/neurips/bioscan1m.png":"qmtF","./research/2023/neurips/d2csg.png":"Ciri","./research/2023/neurips/divinet.png":"f16l","./research/2023/neurips/mvdiffusion.png":"rgWR","./research/2023/neurips/nerfrevisited.png":"6gE5","./research/2023/neurips/neuralfields.jpg":"MkOt","./research/2023/neurips/neuralgraphgen.png":"2qm3","./research/2023/neurips/papr.png":"UXfE","./research/2023/neurips/polydiffuse.png":"yz7y","./research/2023/neurips/puzzlefusion.png":"03Yy","./research/2023/neurips/viper.png":"I23i","./research/2024/iclr/iceformer.png":"PJ7a","./research/2024/iclr/slime.gif":"EhV0","./research/2024/iclr/trajeglish.png":"srT8"};function n(e){return t(r(e))}function r(e){var a=i[e];if(!(a+1))throw new Error("Cannot find module '"+e+"'.");return a}n.keys=function(){return Object.keys(i)},n.resolve=r,e.exports=n,n.id="zzcG"}},["NHnr"]);
//# sourceMappingURL=app.ee4caddff3f4e53308b3.js.map